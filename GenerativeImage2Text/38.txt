2022-12-17 17:45:54,489.489 3099:inference.py:320   <module>(): param:
{'image_path': '../img38/img38_0.png',
 'model_name': 'GIT_BASE',
 'prefix': '',
 'type': 'test_git_inference_single_image'}
2022-12-17 17:45:59,488.488 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.class_embedding                                         will be loaded from image_encoder.class_embedding                                         of shape (768,)
2022-12-17 17:45:59,488.488 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.conv1.weight                                            will be loaded from image_encoder.conv1.weight                                            of shape (768, 3, 16, 16)
2022-12-17 17:45:59,489.489 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_post.bias                                            will be loaded from image_encoder.ln_post.bias                                            of shape (768,)
2022-12-17 17:45:59,489.489 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_post.weight                                          will be loaded from image_encoder.ln_post.weight                                          of shape (768,)
2022-12-17 17:45:59,489.489 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_pre.bias                                             will be loaded from image_encoder.ln_pre.bias                                             of shape (768,)
2022-12-17 17:45:59,489.489 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_pre.weight                                           will be loaded from image_encoder.ln_pre.weight                                           of shape (768,)
2022-12-17 17:45:59,489.489 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.positional_embedding                                    will be loaded from image_encoder.positional_embedding                                    of shape (197, 768)
2022-12-17 17:45:59,489.489 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.proj                                                    will be loaded from image_encoder.proj                                                    of shape (768, 512)
2022-12-17 17:45:59,489.489 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.0.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:45:59,489.489 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.0.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:45:59,489.489 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.0.attn.out_proj.bias              of shape (768,)
2022-12-17 17:45:59,489.489 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.0.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:45:59,489.489 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.0.ln_1.bias                       of shape (768,)
2022-12-17 17:45:59,489.489 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.0.ln_1.weight                     of shape (768,)
2022-12-17 17:45:59,489.489 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.0.ln_2.bias                       of shape (768,)
2022-12-17 17:45:59,489.489 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.0.ln_2.weight                     of shape (768,)
2022-12-17 17:45:59,490.490 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.0.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:45:59,490.490 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.0.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:45:59,490.490 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.0.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:45:59,490.490 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.0.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:45:59,490.490 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.1.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:45:59,490.490 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.1.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:45:59,490.490 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.1.attn.out_proj.bias              of shape (768,)
2022-12-17 17:45:59,490.490 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.1.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:45:59,490.490 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.1.ln_1.bias                       of shape (768,)
2022-12-17 17:45:59,490.490 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.1.ln_1.weight                     of shape (768,)
2022-12-17 17:45:59,490.490 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.1.ln_2.bias                       of shape (768,)
2022-12-17 17:45:59,490.490 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.1.ln_2.weight                     of shape (768,)
2022-12-17 17:45:59,490.490 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.1.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:45:59,490.490 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.1.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:45:59,490.490 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.1.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:45:59,491.491 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.1.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:45:59,491.491 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.in_proj_bias              will be loaded from image_encoder.transformer.resblocks.10.attn.in_proj_bias              of shape (2304,)
2022-12-17 17:45:59,491.491 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.in_proj_weight            will be loaded from image_encoder.transformer.resblocks.10.attn.in_proj_weight            of shape (2304, 768)
2022-12-17 17:45:59,491.491 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.out_proj.bias             will be loaded from image_encoder.transformer.resblocks.10.attn.out_proj.bias             of shape (768,)
2022-12-17 17:45:59,491.491 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.out_proj.weight           will be loaded from image_encoder.transformer.resblocks.10.attn.out_proj.weight           of shape (768, 768)
2022-12-17 17:45:59,491.491 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_1.bias                      will be loaded from image_encoder.transformer.resblocks.10.ln_1.bias                      of shape (768,)
2022-12-17 17:45:59,491.491 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_1.weight                    will be loaded from image_encoder.transformer.resblocks.10.ln_1.weight                    of shape (768,)
2022-12-17 17:45:59,491.491 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_2.bias                      will be loaded from image_encoder.transformer.resblocks.10.ln_2.bias                      of shape (768,)
2022-12-17 17:45:59,491.491 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_2.weight                    will be loaded from image_encoder.transformer.resblocks.10.ln_2.weight                    of shape (768,)
2022-12-17 17:45:59,491.491 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_fc.bias                  will be loaded from image_encoder.transformer.resblocks.10.mlp.c_fc.bias                  of shape (3072,)
2022-12-17 17:45:59,491.491 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_fc.weight                will be loaded from image_encoder.transformer.resblocks.10.mlp.c_fc.weight                of shape (3072, 768)
2022-12-17 17:45:59,491.491 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_proj.bias                will be loaded from image_encoder.transformer.resblocks.10.mlp.c_proj.bias                of shape (768,)
2022-12-17 17:45:59,491.491 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_proj.weight              will be loaded from image_encoder.transformer.resblocks.10.mlp.c_proj.weight              of shape (768, 3072)
2022-12-17 17:45:59,491.491 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.in_proj_bias              will be loaded from image_encoder.transformer.resblocks.11.attn.in_proj_bias              of shape (2304,)
2022-12-17 17:45:59,491.491 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.in_proj_weight            will be loaded from image_encoder.transformer.resblocks.11.attn.in_proj_weight            of shape (2304, 768)
2022-12-17 17:45:59,491.491 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.out_proj.bias             will be loaded from image_encoder.transformer.resblocks.11.attn.out_proj.bias             of shape (768,)
2022-12-17 17:45:59,492.492 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.out_proj.weight           will be loaded from image_encoder.transformer.resblocks.11.attn.out_proj.weight           of shape (768, 768)
2022-12-17 17:45:59,492.492 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_1.bias                      will be loaded from image_encoder.transformer.resblocks.11.ln_1.bias                      of shape (768,)
2022-12-17 17:45:59,492.492 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_1.weight                    will be loaded from image_encoder.transformer.resblocks.11.ln_1.weight                    of shape (768,)
2022-12-17 17:45:59,492.492 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_2.bias                      will be loaded from image_encoder.transformer.resblocks.11.ln_2.bias                      of shape (768,)
2022-12-17 17:45:59,492.492 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_2.weight                    will be loaded from image_encoder.transformer.resblocks.11.ln_2.weight                    of shape (768,)
2022-12-17 17:45:59,492.492 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_fc.bias                  will be loaded from image_encoder.transformer.resblocks.11.mlp.c_fc.bias                  of shape (3072,)
2022-12-17 17:45:59,492.492 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_fc.weight                will be loaded from image_encoder.transformer.resblocks.11.mlp.c_fc.weight                of shape (3072, 768)
2022-12-17 17:45:59,492.492 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_proj.bias                will be loaded from image_encoder.transformer.resblocks.11.mlp.c_proj.bias                of shape (768,)
2022-12-17 17:45:59,492.492 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_proj.weight              will be loaded from image_encoder.transformer.resblocks.11.mlp.c_proj.weight              of shape (768, 3072)
2022-12-17 17:45:59,492.492 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.2.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:45:59,492.492 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.2.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:45:59,492.492 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.2.attn.out_proj.bias              of shape (768,)
2022-12-17 17:45:59,492.492 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.2.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:45:59,492.492 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.2.ln_1.bias                       of shape (768,)
2022-12-17 17:45:59,492.492 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.2.ln_1.weight                     of shape (768,)
2022-12-17 17:45:59,493.493 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.2.ln_2.bias                       of shape (768,)
2022-12-17 17:45:59,493.493 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.2.ln_2.weight                     of shape (768,)
2022-12-17 17:45:59,493.493 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.2.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:45:59,493.493 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.2.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:45:59,493.493 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.2.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:45:59,493.493 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.2.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:45:59,493.493 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.3.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:45:59,493.493 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.3.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:45:59,493.493 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.3.attn.out_proj.bias              of shape (768,)
2022-12-17 17:45:59,493.493 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.3.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:45:59,493.493 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.3.ln_1.bias                       of shape (768,)
2022-12-17 17:45:59,493.493 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.3.ln_1.weight                     of shape (768,)
2022-12-17 17:45:59,493.493 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.3.ln_2.bias                       of shape (768,)
2022-12-17 17:45:59,493.493 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.3.ln_2.weight                     of shape (768,)
2022-12-17 17:45:59,493.493 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.3.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:45:59,493.493 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.3.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:45:59,494.494 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.3.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:45:59,494.494 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.3.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:45:59,494.494 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.4.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:45:59,494.494 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.4.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:45:59,494.494 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.4.attn.out_proj.bias              of shape (768,)
2022-12-17 17:45:59,494.494 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.4.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:45:59,494.494 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.4.ln_1.bias                       of shape (768,)
2022-12-17 17:45:59,494.494 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.4.ln_1.weight                     of shape (768,)
2022-12-17 17:45:59,494.494 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.4.ln_2.bias                       of shape (768,)
2022-12-17 17:45:59,494.494 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.4.ln_2.weight                     of shape (768,)
2022-12-17 17:45:59,494.494 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.4.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:45:59,494.494 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.4.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:45:59,494.494 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.4.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:45:59,494.494 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.4.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:45:59,494.494 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.5.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:45:59,494.494 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.5.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:45:59,495.495 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.5.attn.out_proj.bias              of shape (768,)
2022-12-17 17:45:59,495.495 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.5.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:45:59,495.495 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.5.ln_1.bias                       of shape (768,)
2022-12-17 17:45:59,495.495 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.5.ln_1.weight                     of shape (768,)
2022-12-17 17:45:59,495.495 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.5.ln_2.bias                       of shape (768,)
2022-12-17 17:45:59,495.495 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.5.ln_2.weight                     of shape (768,)
2022-12-17 17:45:59,495.495 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.5.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:45:59,495.495 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.5.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:45:59,495.495 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.5.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:45:59,495.495 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.5.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:45:59,495.495 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.6.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:45:59,495.495 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.6.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:45:59,495.495 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.6.attn.out_proj.bias              of shape (768,)
2022-12-17 17:45:59,495.495 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.6.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:45:59,495.495 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.6.ln_1.bias                       of shape (768,)
2022-12-17 17:45:59,495.495 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.6.ln_1.weight                     of shape (768,)
2022-12-17 17:45:59,495.495 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.6.ln_2.bias                       of shape (768,)
2022-12-17 17:45:59,496.496 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.6.ln_2.weight                     of shape (768,)
2022-12-17 17:45:59,496.496 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.6.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:45:59,496.496 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.6.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:45:59,496.496 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.6.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:45:59,496.496 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.6.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:45:59,496.496 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.7.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:45:59,496.496 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.7.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:45:59,496.496 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.7.attn.out_proj.bias              of shape (768,)
2022-12-17 17:45:59,496.496 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.7.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:45:59,496.496 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.7.ln_1.bias                       of shape (768,)
2022-12-17 17:45:59,496.496 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.7.ln_1.weight                     of shape (768,)
2022-12-17 17:45:59,496.496 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.7.ln_2.bias                       of shape (768,)
2022-12-17 17:45:59,496.496 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.7.ln_2.weight                     of shape (768,)
2022-12-17 17:45:59,496.496 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.7.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:45:59,496.496 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.7.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:45:59,496.496 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.7.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:45:59,497.497 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.7.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:45:59,497.497 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.8.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:45:59,497.497 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.8.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:45:59,497.497 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.8.attn.out_proj.bias              of shape (768,)
2022-12-17 17:45:59,497.497 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.8.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:45:59,497.497 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.8.ln_1.bias                       of shape (768,)
2022-12-17 17:45:59,497.497 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.8.ln_1.weight                     of shape (768,)
2022-12-17 17:45:59,497.497 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.8.ln_2.bias                       of shape (768,)
2022-12-17 17:45:59,497.497 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.8.ln_2.weight                     of shape (768,)
2022-12-17 17:45:59,497.497 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.8.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:45:59,497.497 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.8.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:45:59,497.497 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.8.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:45:59,497.497 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.8.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:45:59,497.497 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.9.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:45:59,497.497 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.9.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:45:59,498.498 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.9.attn.out_proj.bias              of shape (768,)
2022-12-17 17:45:59,498.498 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.9.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:45:59,498.498 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.9.ln_1.bias                       of shape (768,)
2022-12-17 17:45:59,498.498 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.9.ln_1.weight                     of shape (768,)
2022-12-17 17:45:59,498.498 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.9.ln_2.bias                       of shape (768,)
2022-12-17 17:45:59,498.498 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.9.ln_2.weight                     of shape (768,)
2022-12-17 17:45:59,498.498 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.9.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:45:59,498.498 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.9.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:45:59,498.498 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.9.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:45:59,498.498 3099:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.9.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:45:59,498.498 3099:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.layer_norm.bias                                     will be loaded from textual.embedding.layer_norm.bias                                     of shape (768,)
2022-12-17 17:45:59,498.498 3099:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.layer_norm.weight                                   will be loaded from textual.embedding.layer_norm.weight                                   of shape (768,)
2022-12-17 17:45:59,498.498 3099:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.positions.weight                                    will be loaded from textual.embedding.positions.weight                                    of shape (1024, 768)
2022-12-17 17:45:59,498.498 3099:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.words.weight                                        will be loaded from textual.embedding.words.weight                                        of shape (30522, 768)
2022-12-17 17:45:59,498.498 3099:torch_common.py:128 align_and_update_state_dicts(): textual.output.bias                                                   will be loaded from textual.output.bias                                                   of shape (30522,)
2022-12-17 17:45:59,499.499 3099:torch_common.py:128 align_and_update_state_dicts(): textual.output.weight                                                 will be loaded from textual.output.weight                                                 of shape (30522, 768)
2022-12-17 17:45:59,499.499 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.0.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:45:59,499.499 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.0.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:45:59,499.499 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.0.attention.output.dense.bias       of shape (768,)
2022-12-17 17:45:59,499.499 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.0.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:45:59,499.499 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.0.attention.self.key.bias           of shape (768,)
2022-12-17 17:45:59,499.499 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.0.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:45:59,499.499 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.0.attention.self.query.bias         of shape (768,)
2022-12-17 17:45:59,499.499 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.0.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:45:59,499.499 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.0.attention.self.value.bias         of shape (768,)
2022-12-17 17:45:59,499.499 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.0.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:45:59,499.499 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.0.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:45:59,499.499 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.0.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:45:59,499.499 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.0.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:45:59,499.499 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.0.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:45:59,499.499 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.0.output.dense.bias                 of shape (768,)
2022-12-17 17:45:59,500.500 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.dense.weight               will be loaded from textual.transformer.encoder.layer.0.output.dense.weight               of shape (768, 3072)
2022-12-17 17:45:59,500.500 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.1.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:45:59,500.500 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.1.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:45:59,500.500 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.1.attention.output.dense.bias       of shape (768,)
2022-12-17 17:45:59,500.500 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.1.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:45:59,500.500 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.1.attention.self.key.bias           of shape (768,)
2022-12-17 17:45:59,500.500 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.1.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:45:59,500.500 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.1.attention.self.query.bias         of shape (768,)
2022-12-17 17:45:59,500.500 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.1.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:45:59,500.500 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.1.attention.self.value.bias         of shape (768,)
2022-12-17 17:45:59,500.500 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.1.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:45:59,500.500 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.1.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:45:59,500.500 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.1.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:45:59,500.500 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.1.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:45:59,500.500 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.1.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:45:59,500.500 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.1.output.dense.bias                 of shape (768,)
2022-12-17 17:45:59,500.500 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.dense.weight               will be loaded from textual.transformer.encoder.layer.1.output.dense.weight               of shape (768, 3072)
2022-12-17 17:45:59,501.501 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.2.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:45:59,501.501 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.2.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:45:59,501.501 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.2.attention.output.dense.bias       of shape (768,)
2022-12-17 17:45:59,501.501 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.2.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:45:59,501.501 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.2.attention.self.key.bias           of shape (768,)
2022-12-17 17:45:59,501.501 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.2.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:45:59,501.501 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.2.attention.self.query.bias         of shape (768,)
2022-12-17 17:45:59,501.501 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.2.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:45:59,501.501 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.2.attention.self.value.bias         of shape (768,)
2022-12-17 17:45:59,501.501 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.2.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:45:59,501.501 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.2.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:45:59,501.501 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.2.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:45:59,501.501 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.2.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:45:59,501.501 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.2.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:45:59,501.501 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.2.output.dense.bias                 of shape (768,)
2022-12-17 17:45:59,501.501 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.dense.weight               will be loaded from textual.transformer.encoder.layer.2.output.dense.weight               of shape (768, 3072)
2022-12-17 17:45:59,501.501 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.3.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:45:59,502.502 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.3.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:45:59,502.502 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.3.attention.output.dense.bias       of shape (768,)
2022-12-17 17:45:59,502.502 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.3.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:45:59,502.502 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.3.attention.self.key.bias           of shape (768,)
2022-12-17 17:45:59,502.502 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.3.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:45:59,502.502 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.3.attention.self.query.bias         of shape (768,)
2022-12-17 17:45:59,502.502 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.3.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:45:59,502.502 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.3.attention.self.value.bias         of shape (768,)
2022-12-17 17:45:59,502.502 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.3.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:45:59,502.502 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.3.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:45:59,502.502 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.3.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:45:59,502.502 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.3.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:45:59,502.502 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.3.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:45:59,502.502 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.3.output.dense.bias                 of shape (768,)
2022-12-17 17:45:59,503.503 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.dense.weight               will be loaded from textual.transformer.encoder.layer.3.output.dense.weight               of shape (768, 3072)
2022-12-17 17:45:59,503.503 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.4.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:45:59,503.503 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.4.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:45:59,503.503 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.4.attention.output.dense.bias       of shape (768,)
2022-12-17 17:45:59,503.503 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.4.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:45:59,503.503 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.4.attention.self.key.bias           of shape (768,)
2022-12-17 17:45:59,503.503 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.4.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:45:59,503.503 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.4.attention.self.query.bias         of shape (768,)
2022-12-17 17:45:59,503.503 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.4.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:45:59,503.503 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.4.attention.self.value.bias         of shape (768,)
2022-12-17 17:45:59,503.503 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.4.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:45:59,503.503 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.4.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:45:59,503.503 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.4.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:45:59,503.503 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.4.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:45:59,503.503 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.4.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:45:59,504.504 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.4.output.dense.bias                 of shape (768,)
2022-12-17 17:45:59,504.504 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.dense.weight               will be loaded from textual.transformer.encoder.layer.4.output.dense.weight               of shape (768, 3072)
2022-12-17 17:45:59,504.504 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.5.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:45:59,504.504 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.5.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:45:59,504.504 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.5.attention.output.dense.bias       of shape (768,)
2022-12-17 17:45:59,504.504 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.5.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:45:59,504.504 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.5.attention.self.key.bias           of shape (768,)
2022-12-17 17:45:59,504.504 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.5.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:45:59,504.504 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.5.attention.self.query.bias         of shape (768,)
2022-12-17 17:45:59,504.504 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.5.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:45:59,504.504 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.5.attention.self.value.bias         of shape (768,)
2022-12-17 17:45:59,504.504 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.5.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:45:59,504.504 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.5.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:45:59,504.504 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.5.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:45:59,504.504 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.5.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:45:59,504.504 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.5.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:45:59,505.505 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.5.output.dense.bias                 of shape (768,)
2022-12-17 17:45:59,505.505 3099:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.dense.weight               will be loaded from textual.transformer.encoder.layer.5.output.dense.weight               of shape (768, 3072)
2022-12-17 17:45:59,505.505 3099:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.0.bias                                      will be loaded from textual.visual_projection.0.bias                                      of shape (768,)
2022-12-17 17:45:59,505.505 3099:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.0.weight                                    will be loaded from textual.visual_projection.0.weight                                    of shape (768, 768)
2022-12-17 17:45:59,505.505 3099:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.1.bias                                      will be loaded from textual.visual_projection.1.bias                                      of shape (768,)
2022-12-17 17:45:59,505.505 3099:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.1.weight                                    will be loaded from textual.visual_projection.1.weight                                    of shape (768,)
2022-12-17 17:45:59,505.505 3099:torch_common.py:137 align_and_update_state_dicts(): target model param = 258; name matched = 258; loaded = 258
2022-12-17 17:45:59,505.505 3099:torch_common.py:140 align_and_update_state_dicts(): from loaded; ignore = []
2022-12-17 17:45:59,506.506 3099:torch_common.py:82 load_model_state_ignore_mismatch(): unique keys in init dict = ['textual.output.weight']; total = 1
2022-12-17 17:45:59,590.590 3099:torch_common.py:86 load_model_state_ignore_mismatch(): unique key (not initialized) in current model = ['textual.output.weight']
2022-12-17 17:46:22,898.898 3099:inference.py:111 test_git_inference_single_image(): output: a wall on the side of a building
2022-12-17 17:46:36,332.332 3123:inference.py:320   <module>(): param:
{'image_path': '../img38/img38_1.png',
 'model_name': 'GIT_BASE',
 'prefix': '',
 'type': 'test_git_inference_single_image'}
2022-12-17 17:46:48,639.639 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.class_embedding                                         will be loaded from image_encoder.class_embedding                                         of shape (768,)
2022-12-17 17:46:48,639.639 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.conv1.weight                                            will be loaded from image_encoder.conv1.weight                                            of shape (768, 3, 16, 16)
2022-12-17 17:46:48,639.639 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_post.bias                                            will be loaded from image_encoder.ln_post.bias                                            of shape (768,)
2022-12-17 17:46:48,639.639 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_post.weight                                          will be loaded from image_encoder.ln_post.weight                                          of shape (768,)
2022-12-17 17:46:48,639.639 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_pre.bias                                             will be loaded from image_encoder.ln_pre.bias                                             of shape (768,)
2022-12-17 17:46:48,640.640 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_pre.weight                                           will be loaded from image_encoder.ln_pre.weight                                           of shape (768,)
2022-12-17 17:46:48,640.640 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.positional_embedding                                    will be loaded from image_encoder.positional_embedding                                    of shape (197, 768)
2022-12-17 17:46:48,640.640 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.proj                                                    will be loaded from image_encoder.proj                                                    of shape (768, 512)
2022-12-17 17:46:48,640.640 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.0.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:46:48,640.640 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.0.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:46:48,640.640 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.0.attn.out_proj.bias              of shape (768,)
2022-12-17 17:46:48,640.640 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.0.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:46:48,640.640 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.0.ln_1.bias                       of shape (768,)
2022-12-17 17:46:48,640.640 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.0.ln_1.weight                     of shape (768,)
2022-12-17 17:46:48,640.640 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.0.ln_2.bias                       of shape (768,)
2022-12-17 17:46:48,640.640 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.0.ln_2.weight                     of shape (768,)
2022-12-17 17:46:48,640.640 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.0.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:46:48,640.640 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.0.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:46:48,640.640 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.0.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:46:48,640.640 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.0.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:46:48,640.640 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.1.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:46:48,641.641 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.1.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:46:48,641.641 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.1.attn.out_proj.bias              of shape (768,)
2022-12-17 17:46:48,641.641 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.1.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:46:48,641.641 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.1.ln_1.bias                       of shape (768,)
2022-12-17 17:46:48,641.641 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.1.ln_1.weight                     of shape (768,)
2022-12-17 17:46:48,641.641 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.1.ln_2.bias                       of shape (768,)
2022-12-17 17:46:48,641.641 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.1.ln_2.weight                     of shape (768,)
2022-12-17 17:46:48,641.641 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.1.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:46:48,641.641 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.1.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:46:48,641.641 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.1.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:46:48,641.641 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.1.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:46:48,641.641 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.in_proj_bias              will be loaded from image_encoder.transformer.resblocks.10.attn.in_proj_bias              of shape (2304,)
2022-12-17 17:46:48,641.641 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.in_proj_weight            will be loaded from image_encoder.transformer.resblocks.10.attn.in_proj_weight            of shape (2304, 768)
2022-12-17 17:46:48,641.641 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.out_proj.bias             will be loaded from image_encoder.transformer.resblocks.10.attn.out_proj.bias             of shape (768,)
2022-12-17 17:46:48,641.641 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.out_proj.weight           will be loaded from image_encoder.transformer.resblocks.10.attn.out_proj.weight           of shape (768, 768)
2022-12-17 17:46:48,641.641 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_1.bias                      will be loaded from image_encoder.transformer.resblocks.10.ln_1.bias                      of shape (768,)
2022-12-17 17:46:48,641.641 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_1.weight                    will be loaded from image_encoder.transformer.resblocks.10.ln_1.weight                    of shape (768,)
2022-12-17 17:46:48,642.642 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_2.bias                      will be loaded from image_encoder.transformer.resblocks.10.ln_2.bias                      of shape (768,)
2022-12-17 17:46:48,642.642 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_2.weight                    will be loaded from image_encoder.transformer.resblocks.10.ln_2.weight                    of shape (768,)
2022-12-17 17:46:48,642.642 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_fc.bias                  will be loaded from image_encoder.transformer.resblocks.10.mlp.c_fc.bias                  of shape (3072,)
2022-12-17 17:46:48,642.642 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_fc.weight                will be loaded from image_encoder.transformer.resblocks.10.mlp.c_fc.weight                of shape (3072, 768)
2022-12-17 17:46:48,642.642 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_proj.bias                will be loaded from image_encoder.transformer.resblocks.10.mlp.c_proj.bias                of shape (768,)
2022-12-17 17:46:48,642.642 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_proj.weight              will be loaded from image_encoder.transformer.resblocks.10.mlp.c_proj.weight              of shape (768, 3072)
2022-12-17 17:46:48,642.642 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.in_proj_bias              will be loaded from image_encoder.transformer.resblocks.11.attn.in_proj_bias              of shape (2304,)
2022-12-17 17:46:48,642.642 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.in_proj_weight            will be loaded from image_encoder.transformer.resblocks.11.attn.in_proj_weight            of shape (2304, 768)
2022-12-17 17:46:48,642.642 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.out_proj.bias             will be loaded from image_encoder.transformer.resblocks.11.attn.out_proj.bias             of shape (768,)
2022-12-17 17:46:48,642.642 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.out_proj.weight           will be loaded from image_encoder.transformer.resblocks.11.attn.out_proj.weight           of shape (768, 768)
2022-12-17 17:46:48,642.642 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_1.bias                      will be loaded from image_encoder.transformer.resblocks.11.ln_1.bias                      of shape (768,)
2022-12-17 17:46:48,642.642 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_1.weight                    will be loaded from image_encoder.transformer.resblocks.11.ln_1.weight                    of shape (768,)
2022-12-17 17:46:48,642.642 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_2.bias                      will be loaded from image_encoder.transformer.resblocks.11.ln_2.bias                      of shape (768,)
2022-12-17 17:46:48,642.642 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_2.weight                    will be loaded from image_encoder.transformer.resblocks.11.ln_2.weight                    of shape (768,)
2022-12-17 17:46:48,642.642 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_fc.bias                  will be loaded from image_encoder.transformer.resblocks.11.mlp.c_fc.bias                  of shape (3072,)
2022-12-17 17:46:48,642.642 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_fc.weight                will be loaded from image_encoder.transformer.resblocks.11.mlp.c_fc.weight                of shape (3072, 768)
2022-12-17 17:46:48,643.643 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_proj.bias                will be loaded from image_encoder.transformer.resblocks.11.mlp.c_proj.bias                of shape (768,)
2022-12-17 17:46:48,643.643 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_proj.weight              will be loaded from image_encoder.transformer.resblocks.11.mlp.c_proj.weight              of shape (768, 3072)
2022-12-17 17:46:48,643.643 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.2.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:46:48,643.643 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.2.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:46:48,643.643 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.2.attn.out_proj.bias              of shape (768,)
2022-12-17 17:46:48,643.643 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.2.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:46:48,643.643 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.2.ln_1.bias                       of shape (768,)
2022-12-17 17:46:48,643.643 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.2.ln_1.weight                     of shape (768,)
2022-12-17 17:46:48,643.643 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.2.ln_2.bias                       of shape (768,)
2022-12-17 17:46:48,643.643 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.2.ln_2.weight                     of shape (768,)
2022-12-17 17:46:48,643.643 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.2.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:46:48,643.643 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.2.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:46:48,643.643 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.2.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:46:48,643.643 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.2.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:46:48,644.644 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.3.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:46:48,644.644 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.3.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:46:48,644.644 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.3.attn.out_proj.bias              of shape (768,)
2022-12-17 17:46:48,644.644 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.3.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:46:48,644.644 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.3.ln_1.bias                       of shape (768,)
2022-12-17 17:46:48,644.644 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.3.ln_1.weight                     of shape (768,)
2022-12-17 17:46:48,644.644 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.3.ln_2.bias                       of shape (768,)
2022-12-17 17:46:48,644.644 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.3.ln_2.weight                     of shape (768,)
2022-12-17 17:46:48,644.644 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.3.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:46:48,644.644 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.3.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:46:48,644.644 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.3.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:46:48,644.644 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.3.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:46:48,644.644 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.4.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:46:48,644.644 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.4.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:46:48,645.645 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.4.attn.out_proj.bias              of shape (768,)
2022-12-17 17:46:48,645.645 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.4.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:46:48,645.645 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.4.ln_1.bias                       of shape (768,)
2022-12-17 17:46:48,645.645 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.4.ln_1.weight                     of shape (768,)
2022-12-17 17:46:48,645.645 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.4.ln_2.bias                       of shape (768,)
2022-12-17 17:46:48,645.645 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.4.ln_2.weight                     of shape (768,)
2022-12-17 17:46:48,645.645 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.4.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:46:48,645.645 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.4.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:46:48,645.645 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.4.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:46:48,645.645 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.4.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:46:48,645.645 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.5.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:46:48,645.645 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.5.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:46:48,645.645 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.5.attn.out_proj.bias              of shape (768,)
2022-12-17 17:46:48,646.646 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.5.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:46:48,646.646 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.5.ln_1.bias                       of shape (768,)
2022-12-17 17:46:48,646.646 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.5.ln_1.weight                     of shape (768,)
2022-12-17 17:46:48,646.646 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.5.ln_2.bias                       of shape (768,)
2022-12-17 17:46:48,646.646 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.5.ln_2.weight                     of shape (768,)
2022-12-17 17:46:48,646.646 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.5.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:46:48,646.646 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.5.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:46:48,646.646 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.5.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:46:48,646.646 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.5.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:46:48,646.646 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.6.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:46:48,646.646 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.6.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:46:48,646.646 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.6.attn.out_proj.bias              of shape (768,)
2022-12-17 17:46:48,646.646 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.6.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:46:48,646.646 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.6.ln_1.bias                       of shape (768,)
2022-12-17 17:46:48,646.646 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.6.ln_1.weight                     of shape (768,)
2022-12-17 17:46:48,647.647 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.6.ln_2.bias                       of shape (768,)
2022-12-17 17:46:48,647.647 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.6.ln_2.weight                     of shape (768,)
2022-12-17 17:46:48,647.647 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.6.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:46:48,647.647 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.6.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:46:48,647.647 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.6.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:46:48,647.647 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.6.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:46:48,647.647 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.7.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:46:48,647.647 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.7.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:46:48,647.647 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.7.attn.out_proj.bias              of shape (768,)
2022-12-17 17:46:48,647.647 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.7.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:46:48,647.647 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.7.ln_1.bias                       of shape (768,)
2022-12-17 17:46:48,648.648 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.7.ln_1.weight                     of shape (768,)
2022-12-17 17:46:48,648.648 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.7.ln_2.bias                       of shape (768,)
2022-12-17 17:46:48,648.648 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.7.ln_2.weight                     of shape (768,)
2022-12-17 17:46:48,648.648 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.7.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:46:48,648.648 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.7.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:46:48,648.648 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.7.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:46:48,648.648 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.7.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:46:48,648.648 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.8.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:46:48,648.648 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.8.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:46:48,648.648 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.8.attn.out_proj.bias              of shape (768,)
2022-12-17 17:46:48,648.648 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.8.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:46:48,648.648 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.8.ln_1.bias                       of shape (768,)
2022-12-17 17:46:48,648.648 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.8.ln_1.weight                     of shape (768,)
2022-12-17 17:46:48,648.648 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.8.ln_2.bias                       of shape (768,)
2022-12-17 17:46:48,648.648 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.8.ln_2.weight                     of shape (768,)
2022-12-17 17:46:48,649.649 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.8.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:46:48,649.649 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.8.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:46:48,649.649 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.8.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:46:48,649.649 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.8.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:46:48,649.649 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.9.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:46:48,649.649 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.9.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:46:48,649.649 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.9.attn.out_proj.bias              of shape (768,)
2022-12-17 17:46:48,649.649 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.9.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:46:48,649.649 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.9.ln_1.bias                       of shape (768,)
2022-12-17 17:46:48,649.649 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.9.ln_1.weight                     of shape (768,)
2022-12-17 17:46:48,649.649 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.9.ln_2.bias                       of shape (768,)
2022-12-17 17:46:48,649.649 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.9.ln_2.weight                     of shape (768,)
2022-12-17 17:46:48,649.649 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.9.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:46:48,649.649 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.9.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:46:48,649.649 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.9.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:46:48,649.649 3123:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.9.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:46:48,650.650 3123:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.layer_norm.bias                                     will be loaded from textual.embedding.layer_norm.bias                                     of shape (768,)
2022-12-17 17:46:48,650.650 3123:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.layer_norm.weight                                   will be loaded from textual.embedding.layer_norm.weight                                   of shape (768,)
2022-12-17 17:46:48,650.650 3123:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.positions.weight                                    will be loaded from textual.embedding.positions.weight                                    of shape (1024, 768)
2022-12-17 17:46:48,650.650 3123:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.words.weight                                        will be loaded from textual.embedding.words.weight                                        of shape (30522, 768)
2022-12-17 17:46:48,650.650 3123:torch_common.py:128 align_and_update_state_dicts(): textual.output.bias                                                   will be loaded from textual.output.bias                                                   of shape (30522,)
2022-12-17 17:46:48,650.650 3123:torch_common.py:128 align_and_update_state_dicts(): textual.output.weight                                                 will be loaded from textual.output.weight                                                 of shape (30522, 768)
2022-12-17 17:46:48,650.650 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.0.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:46:48,650.650 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.0.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:46:48,650.650 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.0.attention.output.dense.bias       of shape (768,)
2022-12-17 17:46:48,650.650 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.0.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:46:48,650.650 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.0.attention.self.key.bias           of shape (768,)
2022-12-17 17:46:48,650.650 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.0.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:46:48,650.650 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.0.attention.self.query.bias         of shape (768,)
2022-12-17 17:46:48,650.650 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.0.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:46:48,650.650 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.0.attention.self.value.bias         of shape (768,)
2022-12-17 17:46:48,650.650 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.0.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:46:48,651.651 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.0.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:46:48,651.651 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.0.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:46:48,651.651 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.0.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:46:48,651.651 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.0.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:46:48,651.651 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.0.output.dense.bias                 of shape (768,)
2022-12-17 17:46:48,651.651 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.dense.weight               will be loaded from textual.transformer.encoder.layer.0.output.dense.weight               of shape (768, 3072)
2022-12-17 17:46:48,651.651 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.1.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:46:48,651.651 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.1.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:46:48,651.651 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.1.attention.output.dense.bias       of shape (768,)
2022-12-17 17:46:48,651.651 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.1.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:46:48,651.651 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.1.attention.self.key.bias           of shape (768,)
2022-12-17 17:46:48,651.651 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.1.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:46:48,651.651 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.1.attention.self.query.bias         of shape (768,)
2022-12-17 17:46:48,651.651 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.1.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:46:48,651.651 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.1.attention.self.value.bias         of shape (768,)
2022-12-17 17:46:48,651.651 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.1.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:46:48,652.652 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.1.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:46:48,652.652 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.1.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:46:48,652.652 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.1.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:46:48,652.652 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.1.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:46:48,652.652 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.1.output.dense.bias                 of shape (768,)
2022-12-17 17:46:48,652.652 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.dense.weight               will be loaded from textual.transformer.encoder.layer.1.output.dense.weight               of shape (768, 3072)
2022-12-17 17:46:48,652.652 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.2.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:46:48,652.652 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.2.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:46:48,652.652 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.2.attention.output.dense.bias       of shape (768,)
2022-12-17 17:46:48,652.652 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.2.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:46:48,652.652 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.2.attention.self.key.bias           of shape (768,)
2022-12-17 17:46:48,653.653 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.2.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:46:48,653.653 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.2.attention.self.query.bias         of shape (768,)
2022-12-17 17:46:48,653.653 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.2.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:46:48,653.653 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.2.attention.self.value.bias         of shape (768,)
2022-12-17 17:46:48,653.653 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.2.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:46:48,653.653 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.2.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:46:48,653.653 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.2.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:46:48,653.653 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.2.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:46:48,653.653 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.2.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:46:48,653.653 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.2.output.dense.bias                 of shape (768,)
2022-12-17 17:46:48,653.653 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.dense.weight               will be loaded from textual.transformer.encoder.layer.2.output.dense.weight               of shape (768, 3072)
2022-12-17 17:46:48,654.654 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.3.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:46:48,654.654 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.3.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:46:48,654.654 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.3.attention.output.dense.bias       of shape (768,)
2022-12-17 17:46:48,654.654 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.3.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:46:48,654.654 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.3.attention.self.key.bias           of shape (768,)
2022-12-17 17:46:48,654.654 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.3.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:46:48,654.654 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.3.attention.self.query.bias         of shape (768,)
2022-12-17 17:46:48,654.654 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.3.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:46:48,654.654 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.3.attention.self.value.bias         of shape (768,)
2022-12-17 17:46:48,654.654 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.3.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:46:48,654.654 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.3.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:46:48,655.655 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.3.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:46:48,655.655 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.3.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:46:48,655.655 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.3.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:46:48,655.655 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.3.output.dense.bias                 of shape (768,)
2022-12-17 17:46:48,655.655 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.dense.weight               will be loaded from textual.transformer.encoder.layer.3.output.dense.weight               of shape (768, 3072)
2022-12-17 17:46:48,655.655 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.4.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:46:48,655.655 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.4.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:46:48,655.655 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.4.attention.output.dense.bias       of shape (768,)
2022-12-17 17:46:48,655.655 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.4.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:46:48,655.655 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.4.attention.self.key.bias           of shape (768,)
2022-12-17 17:46:48,655.655 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.4.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:46:48,656.656 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.4.attention.self.query.bias         of shape (768,)
2022-12-17 17:46:48,656.656 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.4.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:46:48,656.656 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.4.attention.self.value.bias         of shape (768,)
2022-12-17 17:46:48,656.656 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.4.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:46:48,656.656 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.4.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:46:48,656.656 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.4.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:46:48,656.656 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.4.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:46:48,656.656 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.4.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:46:48,656.656 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.4.output.dense.bias                 of shape (768,)
2022-12-17 17:46:48,656.656 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.dense.weight               will be loaded from textual.transformer.encoder.layer.4.output.dense.weight               of shape (768, 3072)
2022-12-17 17:46:48,657.657 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.5.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:46:48,657.657 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.5.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:46:48,657.657 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.5.attention.output.dense.bias       of shape (768,)
2022-12-17 17:46:48,657.657 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.5.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:46:48,657.657 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.5.attention.self.key.bias           of shape (768,)
2022-12-17 17:46:48,657.657 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.5.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:46:48,657.657 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.5.attention.self.query.bias         of shape (768,)
2022-12-17 17:46:48,657.657 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.5.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:46:48,657.657 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.5.attention.self.value.bias         of shape (768,)
2022-12-17 17:46:48,657.657 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.5.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:46:48,658.658 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.5.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:46:48,658.658 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.5.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:46:48,658.658 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.5.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:46:48,658.658 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.5.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:46:48,658.658 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.5.output.dense.bias                 of shape (768,)
2022-12-17 17:46:48,658.658 3123:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.dense.weight               will be loaded from textual.transformer.encoder.layer.5.output.dense.weight               of shape (768, 3072)
2022-12-17 17:46:48,658.658 3123:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.0.bias                                      will be loaded from textual.visual_projection.0.bias                                      of shape (768,)
2022-12-17 17:46:48,658.658 3123:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.0.weight                                    will be loaded from textual.visual_projection.0.weight                                    of shape (768, 768)
2022-12-17 17:46:48,658.658 3123:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.1.bias                                      will be loaded from textual.visual_projection.1.bias                                      of shape (768,)
2022-12-17 17:46:48,658.658 3123:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.1.weight                                    will be loaded from textual.visual_projection.1.weight                                    of shape (768,)
2022-12-17 17:46:48,658.658 3123:torch_common.py:137 align_and_update_state_dicts(): target model param = 258; name matched = 258; loaded = 258
2022-12-17 17:46:48,659.659 3123:torch_common.py:140 align_and_update_state_dicts(): from loaded; ignore = []
2022-12-17 17:46:48,660.660 3123:torch_common.py:82 load_model_state_ignore_mismatch(): unique keys in init dict = ['textual.output.weight']; total = 1
2022-12-17 17:46:48,812.812 3123:torch_common.py:86 load_model_state_ignore_mismatch(): unique key (not initialized) in current model = ['textual.output.weight']
2022-12-17 17:47:27,296.296 3123:inference.py:111 test_git_inference_single_image(): output: power lines above the street
2022-12-17 17:47:29,224.224 3156:inference.py:320   <module>(): param:
{'image_path': '../img38/img38_2.png',
 'model_name': 'GIT_BASE',
 'prefix': '',
 'type': 'test_git_inference_single_image'}
2022-12-17 17:47:44,604.604 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.class_embedding                                         will be loaded from image_encoder.class_embedding                                         of shape (768,)
2022-12-17 17:47:44,616.616 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.conv1.weight                                            will be loaded from image_encoder.conv1.weight                                            of shape (768, 3, 16, 16)
2022-12-17 17:47:44,616.616 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_post.bias                                            will be loaded from image_encoder.ln_post.bias                                            of shape (768,)
2022-12-17 17:47:44,616.616 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_post.weight                                          will be loaded from image_encoder.ln_post.weight                                          of shape (768,)
2022-12-17 17:47:44,616.616 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_pre.bias                                             will be loaded from image_encoder.ln_pre.bias                                             of shape (768,)
2022-12-17 17:47:44,616.616 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_pre.weight                                           will be loaded from image_encoder.ln_pre.weight                                           of shape (768,)
2022-12-17 17:47:44,616.616 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.positional_embedding                                    will be loaded from image_encoder.positional_embedding                                    of shape (197, 768)
2022-12-17 17:47:44,616.616 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.proj                                                    will be loaded from image_encoder.proj                                                    of shape (768, 512)
2022-12-17 17:47:44,616.616 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.0.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:47:44,616.616 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.0.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:47:44,616.616 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.0.attn.out_proj.bias              of shape (768,)
2022-12-17 17:47:44,616.616 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.0.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:47:44,616.616 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.0.ln_1.bias                       of shape (768,)
2022-12-17 17:47:44,617.617 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.0.ln_1.weight                     of shape (768,)
2022-12-17 17:47:44,617.617 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.0.ln_2.bias                       of shape (768,)
2022-12-17 17:47:44,617.617 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.0.ln_2.weight                     of shape (768,)
2022-12-17 17:47:44,617.617 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.0.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:47:44,617.617 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.0.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:47:44,617.617 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.0.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:47:44,617.617 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.0.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:47:44,617.617 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.1.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:47:44,617.617 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.1.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:47:44,617.617 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.1.attn.out_proj.bias              of shape (768,)
2022-12-17 17:47:44,617.617 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.1.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:47:44,617.617 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.1.ln_1.bias                       of shape (768,)
2022-12-17 17:47:44,617.617 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.1.ln_1.weight                     of shape (768,)
2022-12-17 17:47:44,617.617 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.1.ln_2.bias                       of shape (768,)
2022-12-17 17:47:44,617.617 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.1.ln_2.weight                     of shape (768,)
2022-12-17 17:47:44,618.618 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.1.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:47:44,618.618 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.1.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:47:44,618.618 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.1.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:47:44,618.618 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.1.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:47:44,618.618 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.in_proj_bias              will be loaded from image_encoder.transformer.resblocks.10.attn.in_proj_bias              of shape (2304,)
2022-12-17 17:47:44,618.618 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.in_proj_weight            will be loaded from image_encoder.transformer.resblocks.10.attn.in_proj_weight            of shape (2304, 768)
2022-12-17 17:47:44,618.618 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.out_proj.bias             will be loaded from image_encoder.transformer.resblocks.10.attn.out_proj.bias             of shape (768,)
2022-12-17 17:47:44,618.618 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.out_proj.weight           will be loaded from image_encoder.transformer.resblocks.10.attn.out_proj.weight           of shape (768, 768)
2022-12-17 17:47:44,618.618 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_1.bias                      will be loaded from image_encoder.transformer.resblocks.10.ln_1.bias                      of shape (768,)
2022-12-17 17:47:44,618.618 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_1.weight                    will be loaded from image_encoder.transformer.resblocks.10.ln_1.weight                    of shape (768,)
2022-12-17 17:47:44,618.618 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_2.bias                      will be loaded from image_encoder.transformer.resblocks.10.ln_2.bias                      of shape (768,)
2022-12-17 17:47:44,618.618 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_2.weight                    will be loaded from image_encoder.transformer.resblocks.10.ln_2.weight                    of shape (768,)
2022-12-17 17:47:44,618.618 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_fc.bias                  will be loaded from image_encoder.transformer.resblocks.10.mlp.c_fc.bias                  of shape (3072,)
2022-12-17 17:47:44,618.618 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_fc.weight                will be loaded from image_encoder.transformer.resblocks.10.mlp.c_fc.weight                of shape (3072, 768)
2022-12-17 17:47:44,618.618 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_proj.bias                will be loaded from image_encoder.transformer.resblocks.10.mlp.c_proj.bias                of shape (768,)
2022-12-17 17:47:44,619.619 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_proj.weight              will be loaded from image_encoder.transformer.resblocks.10.mlp.c_proj.weight              of shape (768, 3072)
2022-12-17 17:47:44,619.619 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.in_proj_bias              will be loaded from image_encoder.transformer.resblocks.11.attn.in_proj_bias              of shape (2304,)
2022-12-17 17:47:44,619.619 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.in_proj_weight            will be loaded from image_encoder.transformer.resblocks.11.attn.in_proj_weight            of shape (2304, 768)
2022-12-17 17:47:44,619.619 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.out_proj.bias             will be loaded from image_encoder.transformer.resblocks.11.attn.out_proj.bias             of shape (768,)
2022-12-17 17:47:44,619.619 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.out_proj.weight           will be loaded from image_encoder.transformer.resblocks.11.attn.out_proj.weight           of shape (768, 768)
2022-12-17 17:47:44,619.619 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_1.bias                      will be loaded from image_encoder.transformer.resblocks.11.ln_1.bias                      of shape (768,)
2022-12-17 17:47:44,619.619 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_1.weight                    will be loaded from image_encoder.transformer.resblocks.11.ln_1.weight                    of shape (768,)
2022-12-17 17:47:44,619.619 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_2.bias                      will be loaded from image_encoder.transformer.resblocks.11.ln_2.bias                      of shape (768,)
2022-12-17 17:47:44,619.619 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_2.weight                    will be loaded from image_encoder.transformer.resblocks.11.ln_2.weight                    of shape (768,)
2022-12-17 17:47:44,619.619 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_fc.bias                  will be loaded from image_encoder.transformer.resblocks.11.mlp.c_fc.bias                  of shape (3072,)
2022-12-17 17:47:44,619.619 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_fc.weight                will be loaded from image_encoder.transformer.resblocks.11.mlp.c_fc.weight                of shape (3072, 768)
2022-12-17 17:47:44,619.619 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_proj.bias                will be loaded from image_encoder.transformer.resblocks.11.mlp.c_proj.bias                of shape (768,)
2022-12-17 17:47:44,619.619 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_proj.weight              will be loaded from image_encoder.transformer.resblocks.11.mlp.c_proj.weight              of shape (768, 3072)
2022-12-17 17:47:44,619.619 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.2.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:47:44,619.619 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.2.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:47:44,619.619 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.2.attn.out_proj.bias              of shape (768,)
2022-12-17 17:47:44,620.620 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.2.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:47:44,620.620 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.2.ln_1.bias                       of shape (768,)
2022-12-17 17:47:44,620.620 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.2.ln_1.weight                     of shape (768,)
2022-12-17 17:47:44,620.620 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.2.ln_2.bias                       of shape (768,)
2022-12-17 17:47:44,620.620 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.2.ln_2.weight                     of shape (768,)
2022-12-17 17:47:44,620.620 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.2.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:47:44,620.620 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.2.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:47:44,620.620 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.2.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:47:44,620.620 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.2.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:47:44,620.620 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.3.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:47:44,620.620 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.3.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:47:44,620.620 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.3.attn.out_proj.bias              of shape (768,)
2022-12-17 17:47:44,620.620 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.3.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:47:44,620.620 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.3.ln_1.bias                       of shape (768,)
2022-12-17 17:47:44,620.620 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.3.ln_1.weight                     of shape (768,)
2022-12-17 17:47:44,621.621 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.3.ln_2.bias                       of shape (768,)
2022-12-17 17:47:44,621.621 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.3.ln_2.weight                     of shape (768,)
2022-12-17 17:47:44,621.621 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.3.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:47:44,621.621 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.3.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:47:44,621.621 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.3.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:47:44,621.621 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.3.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:47:44,621.621 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.4.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:47:44,621.621 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.4.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:47:44,621.621 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.4.attn.out_proj.bias              of shape (768,)
2022-12-17 17:47:44,621.621 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.4.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:47:44,621.621 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.4.ln_1.bias                       of shape (768,)
2022-12-17 17:47:44,621.621 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.4.ln_1.weight                     of shape (768,)
2022-12-17 17:47:44,621.621 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.4.ln_2.bias                       of shape (768,)
2022-12-17 17:47:44,621.621 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.4.ln_2.weight                     of shape (768,)
2022-12-17 17:47:44,621.621 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.4.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:47:44,622.622 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.4.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:47:44,622.622 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.4.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:47:44,622.622 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.4.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:47:44,622.622 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.5.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:47:44,622.622 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.5.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:47:44,622.622 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.5.attn.out_proj.bias              of shape (768,)
2022-12-17 17:47:44,622.622 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.5.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:47:44,622.622 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.5.ln_1.bias                       of shape (768,)
2022-12-17 17:47:44,622.622 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.5.ln_1.weight                     of shape (768,)
2022-12-17 17:47:44,622.622 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.5.ln_2.bias                       of shape (768,)
2022-12-17 17:47:44,622.622 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.5.ln_2.weight                     of shape (768,)
2022-12-17 17:47:44,622.622 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.5.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:47:44,622.622 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.5.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:47:44,622.622 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.5.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:47:44,622.622 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.5.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:47:44,622.622 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.6.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:47:44,623.623 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.6.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:47:44,623.623 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.6.attn.out_proj.bias              of shape (768,)
2022-12-17 17:47:44,623.623 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.6.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:47:44,623.623 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.6.ln_1.bias                       of shape (768,)
2022-12-17 17:47:44,623.623 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.6.ln_1.weight                     of shape (768,)
2022-12-17 17:47:44,623.623 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.6.ln_2.bias                       of shape (768,)
2022-12-17 17:47:44,623.623 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.6.ln_2.weight                     of shape (768,)
2022-12-17 17:47:44,623.623 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.6.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:47:44,623.623 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.6.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:47:44,623.623 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.6.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:47:44,623.623 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.6.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:47:44,623.623 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.7.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:47:44,623.623 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.7.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:47:44,623.623 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.7.attn.out_proj.bias              of shape (768,)
2022-12-17 17:47:44,623.623 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.7.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:47:44,624.624 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.7.ln_1.bias                       of shape (768,)
2022-12-17 17:47:44,624.624 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.7.ln_1.weight                     of shape (768,)
2022-12-17 17:47:44,624.624 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.7.ln_2.bias                       of shape (768,)
2022-12-17 17:47:44,624.624 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.7.ln_2.weight                     of shape (768,)
2022-12-17 17:47:44,624.624 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.7.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:47:44,624.624 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.7.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:47:44,624.624 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.7.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:47:44,624.624 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.7.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:47:44,624.624 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.8.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:47:44,624.624 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.8.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:47:44,624.624 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.8.attn.out_proj.bias              of shape (768,)
2022-12-17 17:47:44,624.624 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.8.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:47:44,624.624 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.8.ln_1.bias                       of shape (768,)
2022-12-17 17:47:44,624.624 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.8.ln_1.weight                     of shape (768,)
2022-12-17 17:47:44,624.624 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.8.ln_2.bias                       of shape (768,)
2022-12-17 17:47:44,624.624 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.8.ln_2.weight                     of shape (768,)
2022-12-17 17:47:44,625.625 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.8.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:47:44,625.625 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.8.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:47:44,625.625 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.8.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:47:44,625.625 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.8.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:47:44,625.625 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.9.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:47:44,625.625 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.9.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:47:44,625.625 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.9.attn.out_proj.bias              of shape (768,)
2022-12-17 17:47:44,625.625 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.9.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:47:44,625.625 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.9.ln_1.bias                       of shape (768,)
2022-12-17 17:47:44,625.625 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.9.ln_1.weight                     of shape (768,)
2022-12-17 17:47:44,625.625 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.9.ln_2.bias                       of shape (768,)
2022-12-17 17:47:44,625.625 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.9.ln_2.weight                     of shape (768,)
2022-12-17 17:47:44,625.625 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.9.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:47:44,625.625 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.9.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:47:44,625.625 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.9.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:47:44,626.626 3156:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.9.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:47:44,626.626 3156:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.layer_norm.bias                                     will be loaded from textual.embedding.layer_norm.bias                                     of shape (768,)
2022-12-17 17:47:44,626.626 3156:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.layer_norm.weight                                   will be loaded from textual.embedding.layer_norm.weight                                   of shape (768,)
2022-12-17 17:47:44,626.626 3156:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.positions.weight                                    will be loaded from textual.embedding.positions.weight                                    of shape (1024, 768)
2022-12-17 17:47:44,626.626 3156:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.words.weight                                        will be loaded from textual.embedding.words.weight                                        of shape (30522, 768)
2022-12-17 17:47:44,626.626 3156:torch_common.py:128 align_and_update_state_dicts(): textual.output.bias                                                   will be loaded from textual.output.bias                                                   of shape (30522,)
2022-12-17 17:47:44,626.626 3156:torch_common.py:128 align_and_update_state_dicts(): textual.output.weight                                                 will be loaded from textual.output.weight                                                 of shape (30522, 768)
2022-12-17 17:47:44,626.626 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.0.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:47:44,626.626 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.0.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:47:44,626.626 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.0.attention.output.dense.bias       of shape (768,)
2022-12-17 17:47:44,626.626 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.0.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:47:44,626.626 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.0.attention.self.key.bias           of shape (768,)
2022-12-17 17:47:44,626.626 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.0.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:47:44,626.626 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.0.attention.self.query.bias         of shape (768,)
2022-12-17 17:47:44,626.626 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.0.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:47:44,627.627 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.0.attention.self.value.bias         of shape (768,)
2022-12-17 17:47:44,627.627 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.0.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:47:44,627.627 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.0.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:47:44,627.627 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.0.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:47:44,627.627 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.0.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:47:44,627.627 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.0.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:47:44,627.627 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.0.output.dense.bias                 of shape (768,)
2022-12-17 17:47:44,627.627 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.dense.weight               will be loaded from textual.transformer.encoder.layer.0.output.dense.weight               of shape (768, 3072)
2022-12-17 17:47:44,627.627 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.1.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:47:44,627.627 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.1.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:47:44,627.627 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.1.attention.output.dense.bias       of shape (768,)
2022-12-17 17:47:44,627.627 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.1.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:47:44,627.627 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.1.attention.self.key.bias           of shape (768,)
2022-12-17 17:47:44,627.627 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.1.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:47:44,627.627 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.1.attention.self.query.bias         of shape (768,)
2022-12-17 17:47:44,628.628 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.1.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:47:44,628.628 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.1.attention.self.value.bias         of shape (768,)
2022-12-17 17:47:44,628.628 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.1.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:47:44,628.628 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.1.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:47:44,628.628 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.1.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:47:44,628.628 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.1.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:47:44,628.628 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.1.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:47:44,628.628 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.1.output.dense.bias                 of shape (768,)
2022-12-17 17:47:44,628.628 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.dense.weight               will be loaded from textual.transformer.encoder.layer.1.output.dense.weight               of shape (768, 3072)
2022-12-17 17:47:44,628.628 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.2.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:47:44,628.628 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.2.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:47:44,628.628 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.2.attention.output.dense.bias       of shape (768,)
2022-12-17 17:47:44,628.628 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.2.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:47:44,628.628 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.2.attention.self.key.bias           of shape (768,)
2022-12-17 17:47:44,628.628 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.2.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:47:44,628.628 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.2.attention.self.query.bias         of shape (768,)
2022-12-17 17:47:44,629.629 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.2.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:47:44,629.629 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.2.attention.self.value.bias         of shape (768,)
2022-12-17 17:47:44,629.629 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.2.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:47:44,629.629 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.2.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:47:44,629.629 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.2.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:47:44,629.629 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.2.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:47:44,629.629 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.2.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:47:44,629.629 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.2.output.dense.bias                 of shape (768,)
2022-12-17 17:47:44,629.629 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.dense.weight               will be loaded from textual.transformer.encoder.layer.2.output.dense.weight               of shape (768, 3072)
2022-12-17 17:47:44,629.629 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.3.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:47:44,629.629 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.3.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:47:44,629.629 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.3.attention.output.dense.bias       of shape (768,)
2022-12-17 17:47:44,629.629 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.3.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:47:44,629.629 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.3.attention.self.key.bias           of shape (768,)
2022-12-17 17:47:44,629.629 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.3.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:47:44,629.629 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.3.attention.self.query.bias         of shape (768,)
2022-12-17 17:47:44,630.630 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.3.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:47:44,630.630 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.3.attention.self.value.bias         of shape (768,)
2022-12-17 17:47:44,630.630 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.3.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:47:44,630.630 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.3.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:47:44,630.630 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.3.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:47:44,630.630 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.3.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:47:44,630.630 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.3.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:47:44,630.630 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.3.output.dense.bias                 of shape (768,)
2022-12-17 17:47:44,630.630 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.dense.weight               will be loaded from textual.transformer.encoder.layer.3.output.dense.weight               of shape (768, 3072)
2022-12-17 17:47:44,630.630 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.4.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:47:44,630.630 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.4.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:47:44,630.630 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.4.attention.output.dense.bias       of shape (768,)
2022-12-17 17:47:44,630.630 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.4.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:47:44,630.630 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.4.attention.self.key.bias           of shape (768,)
2022-12-17 17:47:44,630.630 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.4.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:47:44,630.630 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.4.attention.self.query.bias         of shape (768,)
2022-12-17 17:47:44,630.630 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.4.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:47:44,631.631 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.4.attention.self.value.bias         of shape (768,)
2022-12-17 17:47:44,631.631 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.4.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:47:44,631.631 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.4.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:47:44,631.631 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.4.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:47:44,631.631 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.4.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:47:44,631.631 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.4.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:47:44,631.631 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.4.output.dense.bias                 of shape (768,)
2022-12-17 17:47:44,631.631 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.dense.weight               will be loaded from textual.transformer.encoder.layer.4.output.dense.weight               of shape (768, 3072)
2022-12-17 17:47:44,631.631 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.5.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:47:44,631.631 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.5.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:47:44,631.631 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.5.attention.output.dense.bias       of shape (768,)
2022-12-17 17:47:44,631.631 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.5.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:47:44,631.631 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.5.attention.self.key.bias           of shape (768,)
2022-12-17 17:47:44,631.631 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.5.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:47:44,631.631 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.5.attention.self.query.bias         of shape (768,)
2022-12-17 17:47:44,632.632 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.5.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:47:44,632.632 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.5.attention.self.value.bias         of shape (768,)
2022-12-17 17:47:44,632.632 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.5.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:47:44,632.632 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.5.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:47:44,632.632 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.5.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:47:44,632.632 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.5.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:47:44,632.632 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.5.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:47:44,632.632 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.5.output.dense.bias                 of shape (768,)
2022-12-17 17:47:44,632.632 3156:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.dense.weight               will be loaded from textual.transformer.encoder.layer.5.output.dense.weight               of shape (768, 3072)
2022-12-17 17:47:44,632.632 3156:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.0.bias                                      will be loaded from textual.visual_projection.0.bias                                      of shape (768,)
2022-12-17 17:47:44,632.632 3156:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.0.weight                                    will be loaded from textual.visual_projection.0.weight                                    of shape (768, 768)
2022-12-17 17:47:44,632.632 3156:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.1.bias                                      will be loaded from textual.visual_projection.1.bias                                      of shape (768,)
2022-12-17 17:47:44,632.632 3156:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.1.weight                                    will be loaded from textual.visual_projection.1.weight                                    of shape (768,)
2022-12-17 17:47:44,632.632 3156:torch_common.py:137 align_and_update_state_dicts(): target model param = 258; name matched = 258; loaded = 258
2022-12-17 17:47:44,637.637 3156:torch_common.py:140 align_and_update_state_dicts(): from loaded; ignore = []
2022-12-17 17:47:44,643.643 3156:torch_common.py:82 load_model_state_ignore_mismatch(): unique keys in init dict = ['textual.output.weight']; total = 1
2022-12-17 17:47:46,789.789 3156:torch_common.py:86 load_model_state_ignore_mismatch(): unique key (not initialized) in current model = ['textual.output.weight']
2022-12-17 17:52:59,314.314 3156:inference.py:111 test_git_inference_single_image(): output: the road is tarmacked
2022-12-17 17:53:10,511.511 3178:inference.py:320   <module>(): param:
{'image_path': '../img38/img38_3.png',
 'model_name': 'GIT_BASE',
 'prefix': '',
 'type': 'test_git_inference_single_image'}
2022-12-17 17:53:43,071.071 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.class_embedding                                         will be loaded from image_encoder.class_embedding                                         of shape (768,)
2022-12-17 17:53:43,080.080 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.conv1.weight                                            will be loaded from image_encoder.conv1.weight                                            of shape (768, 3, 16, 16)
2022-12-17 17:53:43,081.081 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_post.bias                                            will be loaded from image_encoder.ln_post.bias                                            of shape (768,)
2022-12-17 17:53:43,081.081 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_post.weight                                          will be loaded from image_encoder.ln_post.weight                                          of shape (768,)
2022-12-17 17:53:43,081.081 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_pre.bias                                             will be loaded from image_encoder.ln_pre.bias                                             of shape (768,)
2022-12-17 17:53:43,081.081 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_pre.weight                                           will be loaded from image_encoder.ln_pre.weight                                           of shape (768,)
2022-12-17 17:53:43,082.082 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.positional_embedding                                    will be loaded from image_encoder.positional_embedding                                    of shape (197, 768)
2022-12-17 17:53:43,082.082 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.proj                                                    will be loaded from image_encoder.proj                                                    of shape (768, 512)
2022-12-17 17:53:43,082.082 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.0.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:53:43,082.082 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.0.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:53:43,082.082 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.0.attn.out_proj.bias              of shape (768,)
2022-12-17 17:53:43,083.083 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.0.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:53:43,083.083 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.0.ln_1.bias                       of shape (768,)
2022-12-17 17:53:43,083.083 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.0.ln_1.weight                     of shape (768,)
2022-12-17 17:53:43,083.083 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.0.ln_2.bias                       of shape (768,)
2022-12-17 17:53:43,083.083 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.0.ln_2.weight                     of shape (768,)
2022-12-17 17:53:43,084.084 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.0.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:53:43,084.084 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.0.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:53:43,084.084 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.0.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:53:43,084.084 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.0.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:53:43,085.085 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.1.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:53:43,085.085 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.1.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:53:43,085.085 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.1.attn.out_proj.bias              of shape (768,)
2022-12-17 17:53:43,086.086 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.1.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:53:43,086.086 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.1.ln_1.bias                       of shape (768,)
2022-12-17 17:53:43,086.086 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.1.ln_1.weight                     of shape (768,)
2022-12-17 17:53:43,086.086 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.1.ln_2.bias                       of shape (768,)
2022-12-17 17:53:43,087.087 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.1.ln_2.weight                     of shape (768,)
2022-12-17 17:53:43,087.087 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.1.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:53:43,087.087 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.1.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:53:43,087.087 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.1.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:53:43,088.088 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.1.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:53:43,088.088 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.in_proj_bias              will be loaded from image_encoder.transformer.resblocks.10.attn.in_proj_bias              of shape (2304,)
2022-12-17 17:53:43,088.088 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.in_proj_weight            will be loaded from image_encoder.transformer.resblocks.10.attn.in_proj_weight            of shape (2304, 768)
2022-12-17 17:53:43,089.089 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.out_proj.bias             will be loaded from image_encoder.transformer.resblocks.10.attn.out_proj.bias             of shape (768,)
2022-12-17 17:53:43,089.089 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.out_proj.weight           will be loaded from image_encoder.transformer.resblocks.10.attn.out_proj.weight           of shape (768, 768)
2022-12-17 17:53:43,089.089 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_1.bias                      will be loaded from image_encoder.transformer.resblocks.10.ln_1.bias                      of shape (768,)
2022-12-17 17:53:43,089.089 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_1.weight                    will be loaded from image_encoder.transformer.resblocks.10.ln_1.weight                    of shape (768,)
2022-12-17 17:53:43,089.089 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_2.bias                      will be loaded from image_encoder.transformer.resblocks.10.ln_2.bias                      of shape (768,)
2022-12-17 17:53:43,090.090 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_2.weight                    will be loaded from image_encoder.transformer.resblocks.10.ln_2.weight                    of shape (768,)
2022-12-17 17:53:43,090.090 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_fc.bias                  will be loaded from image_encoder.transformer.resblocks.10.mlp.c_fc.bias                  of shape (3072,)
2022-12-17 17:53:43,090.090 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_fc.weight                will be loaded from image_encoder.transformer.resblocks.10.mlp.c_fc.weight                of shape (3072, 768)
2022-12-17 17:53:43,090.090 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_proj.bias                will be loaded from image_encoder.transformer.resblocks.10.mlp.c_proj.bias                of shape (768,)
2022-12-17 17:53:43,090.090 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_proj.weight              will be loaded from image_encoder.transformer.resblocks.10.mlp.c_proj.weight              of shape (768, 3072)
2022-12-17 17:53:43,090.090 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.in_proj_bias              will be loaded from image_encoder.transformer.resblocks.11.attn.in_proj_bias              of shape (2304,)
2022-12-17 17:53:43,091.091 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.in_proj_weight            will be loaded from image_encoder.transformer.resblocks.11.attn.in_proj_weight            of shape (2304, 768)
2022-12-17 17:53:43,091.091 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.out_proj.bias             will be loaded from image_encoder.transformer.resblocks.11.attn.out_proj.bias             of shape (768,)
2022-12-17 17:53:43,091.091 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.out_proj.weight           will be loaded from image_encoder.transformer.resblocks.11.attn.out_proj.weight           of shape (768, 768)
2022-12-17 17:53:43,091.091 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_1.bias                      will be loaded from image_encoder.transformer.resblocks.11.ln_1.bias                      of shape (768,)
2022-12-17 17:53:43,091.091 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_1.weight                    will be loaded from image_encoder.transformer.resblocks.11.ln_1.weight                    of shape (768,)
2022-12-17 17:53:43,092.092 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_2.bias                      will be loaded from image_encoder.transformer.resblocks.11.ln_2.bias                      of shape (768,)
2022-12-17 17:53:43,092.092 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_2.weight                    will be loaded from image_encoder.transformer.resblocks.11.ln_2.weight                    of shape (768,)
2022-12-17 17:53:43,093.093 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_fc.bias                  will be loaded from image_encoder.transformer.resblocks.11.mlp.c_fc.bias                  of shape (3072,)
2022-12-17 17:53:43,093.093 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_fc.weight                will be loaded from image_encoder.transformer.resblocks.11.mlp.c_fc.weight                of shape (3072, 768)
2022-12-17 17:53:43,093.093 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_proj.bias                will be loaded from image_encoder.transformer.resblocks.11.mlp.c_proj.bias                of shape (768,)
2022-12-17 17:53:43,093.093 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_proj.weight              will be loaded from image_encoder.transformer.resblocks.11.mlp.c_proj.weight              of shape (768, 3072)
2022-12-17 17:53:43,093.093 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.2.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:53:43,094.094 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.2.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:53:43,094.094 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.2.attn.out_proj.bias              of shape (768,)
2022-12-17 17:53:43,095.095 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.2.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:53:43,096.096 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.2.ln_1.bias                       of shape (768,)
2022-12-17 17:53:43,097.097 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.2.ln_1.weight                     of shape (768,)
2022-12-17 17:53:43,097.097 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.2.ln_2.bias                       of shape (768,)
2022-12-17 17:53:43,097.097 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.2.ln_2.weight                     of shape (768,)
2022-12-17 17:53:43,098.098 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.2.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:53:43,098.098 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.2.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:53:43,098.098 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.2.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:53:43,098.098 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.2.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:53:43,099.099 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.3.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:53:43,099.099 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.3.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:53:43,099.099 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.3.attn.out_proj.bias              of shape (768,)
2022-12-17 17:53:43,099.099 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.3.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:53:43,099.099 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.3.ln_1.bias                       of shape (768,)
2022-12-17 17:53:43,100.100 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.3.ln_1.weight                     of shape (768,)
2022-12-17 17:53:43,100.100 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.3.ln_2.bias                       of shape (768,)
2022-12-17 17:53:43,100.100 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.3.ln_2.weight                     of shape (768,)
2022-12-17 17:53:43,100.100 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.3.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:53:43,101.101 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.3.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:53:43,101.101 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.3.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:53:43,101.101 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.3.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:53:43,101.101 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.4.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:53:43,102.102 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.4.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:53:43,102.102 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.4.attn.out_proj.bias              of shape (768,)
2022-12-17 17:53:43,102.102 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.4.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:53:43,102.102 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.4.ln_1.bias                       of shape (768,)
2022-12-17 17:53:43,103.103 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.4.ln_1.weight                     of shape (768,)
2022-12-17 17:53:43,103.103 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.4.ln_2.bias                       of shape (768,)
2022-12-17 17:53:43,103.103 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.4.ln_2.weight                     of shape (768,)
2022-12-17 17:53:43,103.103 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.4.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:53:43,104.104 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.4.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:53:43,104.104 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.4.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:53:43,104.104 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.4.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:53:43,104.104 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.5.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:53:43,105.105 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.5.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:53:43,105.105 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.5.attn.out_proj.bias              of shape (768,)
2022-12-17 17:53:43,105.105 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.5.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:53:43,105.105 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.5.ln_1.bias                       of shape (768,)
2022-12-17 17:53:43,105.105 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.5.ln_1.weight                     of shape (768,)
2022-12-17 17:53:43,106.106 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.5.ln_2.bias                       of shape (768,)
2022-12-17 17:53:43,106.106 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.5.ln_2.weight                     of shape (768,)
2022-12-17 17:53:43,106.106 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.5.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:53:43,106.106 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.5.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:53:43,107.107 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.5.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:53:43,107.107 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.5.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:53:43,107.107 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.6.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:53:43,107.107 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.6.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:53:43,108.108 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.6.attn.out_proj.bias              of shape (768,)
2022-12-17 17:53:43,108.108 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.6.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:53:43,108.108 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.6.ln_1.bias                       of shape (768,)
2022-12-17 17:53:43,108.108 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.6.ln_1.weight                     of shape (768,)
2022-12-17 17:53:43,108.108 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.6.ln_2.bias                       of shape (768,)
2022-12-17 17:53:43,109.109 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.6.ln_2.weight                     of shape (768,)
2022-12-17 17:53:43,109.109 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.6.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:53:43,109.109 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.6.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:53:43,109.109 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.6.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:53:43,110.110 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.6.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:53:43,110.110 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.7.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:53:43,110.110 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.7.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:53:43,110.110 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.7.attn.out_proj.bias              of shape (768,)
2022-12-17 17:53:43,110.110 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.7.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:53:43,110.110 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.7.ln_1.bias                       of shape (768,)
2022-12-17 17:53:43,111.111 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.7.ln_1.weight                     of shape (768,)
2022-12-17 17:53:43,111.111 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.7.ln_2.bias                       of shape (768,)
2022-12-17 17:53:43,111.111 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.7.ln_2.weight                     of shape (768,)
2022-12-17 17:53:43,111.111 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.7.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:53:43,112.112 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.7.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:53:43,112.112 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.7.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:53:43,112.112 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.7.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:53:43,112.112 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.8.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:53:43,113.113 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.8.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:53:43,113.113 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.8.attn.out_proj.bias              of shape (768,)
2022-12-17 17:53:43,113.113 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.8.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:53:43,113.113 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.8.ln_1.bias                       of shape (768,)
2022-12-17 17:53:43,113.113 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.8.ln_1.weight                     of shape (768,)
2022-12-17 17:53:43,114.114 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.8.ln_2.bias                       of shape (768,)
2022-12-17 17:53:43,114.114 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.8.ln_2.weight                     of shape (768,)
2022-12-17 17:53:43,114.114 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.8.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:53:43,114.114 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.8.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:53:43,115.115 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.8.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:53:43,115.115 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.8.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:53:43,115.115 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.9.attn.in_proj_bias               of shape (2304,)
2022-12-17 17:53:43,115.115 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.9.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 17:53:43,115.115 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.9.attn.out_proj.bias              of shape (768,)
2022-12-17 17:53:43,116.116 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.9.attn.out_proj.weight            of shape (768, 768)
2022-12-17 17:53:43,116.116 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.9.ln_1.bias                       of shape (768,)
2022-12-17 17:53:43,116.116 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.9.ln_1.weight                     of shape (768,)
2022-12-17 17:53:43,116.116 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.9.ln_2.bias                       of shape (768,)
2022-12-17 17:53:43,117.117 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.9.ln_2.weight                     of shape (768,)
2022-12-17 17:53:43,117.117 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.9.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 17:53:43,117.117 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.9.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 17:53:43,117.117 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.9.mlp.c_proj.bias                 of shape (768,)
2022-12-17 17:53:43,117.117 3178:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.9.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 17:53:43,118.118 3178:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.layer_norm.bias                                     will be loaded from textual.embedding.layer_norm.bias                                     of shape (768,)
2022-12-17 17:53:43,118.118 3178:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.layer_norm.weight                                   will be loaded from textual.embedding.layer_norm.weight                                   of shape (768,)
2022-12-17 17:53:43,118.118 3178:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.positions.weight                                    will be loaded from textual.embedding.positions.weight                                    of shape (1024, 768)
2022-12-17 17:53:43,118.118 3178:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.words.weight                                        will be loaded from textual.embedding.words.weight                                        of shape (30522, 768)
2022-12-17 17:53:43,118.118 3178:torch_common.py:128 align_and_update_state_dicts(): textual.output.bias                                                   will be loaded from textual.output.bias                                                   of shape (30522,)
2022-12-17 17:53:43,119.119 3178:torch_common.py:128 align_and_update_state_dicts(): textual.output.weight                                                 will be loaded from textual.output.weight                                                 of shape (30522, 768)
2022-12-17 17:53:43,119.119 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.0.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:53:43,119.119 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.0.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:53:43,119.119 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.0.attention.output.dense.bias       of shape (768,)
2022-12-17 17:53:43,119.119 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.0.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:53:43,120.120 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.0.attention.self.key.bias           of shape (768,)
2022-12-17 17:53:43,120.120 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.0.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:53:43,120.120 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.0.attention.self.query.bias         of shape (768,)
2022-12-17 17:53:43,120.120 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.0.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:53:43,120.120 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.0.attention.self.value.bias         of shape (768,)
2022-12-17 17:53:43,121.121 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.0.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:53:43,121.121 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.0.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:53:43,121.121 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.0.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:53:43,121.121 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.0.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:53:43,122.122 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.0.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:53:43,122.122 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.0.output.dense.bias                 of shape (768,)
2022-12-17 17:53:43,122.122 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.dense.weight               will be loaded from textual.transformer.encoder.layer.0.output.dense.weight               of shape (768, 3072)
2022-12-17 17:53:43,122.122 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.1.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:53:43,122.122 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.1.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:53:43,123.123 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.1.attention.output.dense.bias       of shape (768,)
2022-12-17 17:53:43,123.123 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.1.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:53:43,123.123 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.1.attention.self.key.bias           of shape (768,)
2022-12-17 17:53:43,123.123 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.1.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:53:43,124.124 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.1.attention.self.query.bias         of shape (768,)
2022-12-17 17:53:43,124.124 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.1.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:53:43,124.124 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.1.attention.self.value.bias         of shape (768,)
2022-12-17 17:53:43,124.124 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.1.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:53:43,124.124 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.1.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:53:43,125.125 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.1.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:53:43,125.125 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.1.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:53:43,125.125 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.1.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:53:43,125.125 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.1.output.dense.bias                 of shape (768,)
2022-12-17 17:53:43,126.126 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.dense.weight               will be loaded from textual.transformer.encoder.layer.1.output.dense.weight               of shape (768, 3072)
2022-12-17 17:53:43,126.126 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.2.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:53:43,126.126 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.2.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:53:43,126.126 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.2.attention.output.dense.bias       of shape (768,)
2022-12-17 17:53:43,126.126 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.2.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:53:43,127.127 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.2.attention.self.key.bias           of shape (768,)
2022-12-17 17:53:43,127.127 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.2.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:53:43,127.127 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.2.attention.self.query.bias         of shape (768,)
2022-12-17 17:53:43,127.127 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.2.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:53:43,128.128 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.2.attention.self.value.bias         of shape (768,)
2022-12-17 17:53:43,128.128 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.2.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:53:43,128.128 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.2.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:53:43,128.128 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.2.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:53:43,128.128 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.2.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:53:43,129.129 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.2.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:53:43,129.129 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.2.output.dense.bias                 of shape (768,)
2022-12-17 17:53:43,129.129 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.dense.weight               will be loaded from textual.transformer.encoder.layer.2.output.dense.weight               of shape (768, 3072)
2022-12-17 17:53:43,129.129 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.3.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:53:43,129.129 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.3.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:53:43,130.130 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.3.attention.output.dense.bias       of shape (768,)
2022-12-17 17:53:43,130.130 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.3.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:53:43,130.130 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.3.attention.self.key.bias           of shape (768,)
2022-12-17 17:53:43,130.130 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.3.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:53:43,130.130 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.3.attention.self.query.bias         of shape (768,)
2022-12-17 17:53:43,131.131 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.3.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:53:43,131.131 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.3.attention.self.value.bias         of shape (768,)
2022-12-17 17:53:43,131.131 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.3.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:53:43,131.131 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.3.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:53:43,131.131 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.3.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:53:43,132.132 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.3.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:53:43,132.132 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.3.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:53:43,132.132 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.3.output.dense.bias                 of shape (768,)
2022-12-17 17:53:43,132.132 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.dense.weight               will be loaded from textual.transformer.encoder.layer.3.output.dense.weight               of shape (768, 3072)
2022-12-17 17:53:43,133.133 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.4.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:53:43,133.133 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.4.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:53:43,133.133 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.4.attention.output.dense.bias       of shape (768,)
2022-12-17 17:53:43,133.133 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.4.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:53:43,133.133 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.4.attention.self.key.bias           of shape (768,)
2022-12-17 17:53:43,134.134 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.4.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:53:43,134.134 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.4.attention.self.query.bias         of shape (768,)
2022-12-17 17:53:43,134.134 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.4.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:53:43,134.134 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.4.attention.self.value.bias         of shape (768,)
2022-12-17 17:53:43,135.135 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.4.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:53:43,135.135 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.4.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:53:43,135.135 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.4.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:53:43,135.135 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.4.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:53:43,135.135 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.4.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:53:43,136.136 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.4.output.dense.bias                 of shape (768,)
2022-12-17 17:53:43,136.136 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.dense.weight               will be loaded from textual.transformer.encoder.layer.4.output.dense.weight               of shape (768, 3072)
2022-12-17 17:53:43,136.136 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.5.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 17:53:43,136.136 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.5.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 17:53:43,136.136 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.5.attention.output.dense.bias       of shape (768,)
2022-12-17 17:53:43,137.137 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.5.attention.output.dense.weight     of shape (768, 768)
2022-12-17 17:53:43,137.137 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.5.attention.self.key.bias           of shape (768,)
2022-12-17 17:53:43,137.137 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.5.attention.self.key.weight         of shape (768, 768)
2022-12-17 17:53:43,137.137 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.5.attention.self.query.bias         of shape (768,)
2022-12-17 17:53:43,137.137 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.5.attention.self.query.weight       of shape (768, 768)
2022-12-17 17:53:43,138.138 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.5.attention.self.value.bias         of shape (768,)
2022-12-17 17:53:43,138.138 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.5.attention.self.value.weight       of shape (768, 768)
2022-12-17 17:53:43,138.138 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.5.intermediate.dense.bias           of shape (3072,)
2022-12-17 17:53:43,138.138 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.5.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 17:53:43,138.138 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.5.output.LayerNorm.bias             of shape (768,)
2022-12-17 17:53:43,139.139 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.5.output.LayerNorm.weight           of shape (768,)
2022-12-17 17:53:43,139.139 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.5.output.dense.bias                 of shape (768,)
2022-12-17 17:53:43,139.139 3178:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.dense.weight               will be loaded from textual.transformer.encoder.layer.5.output.dense.weight               of shape (768, 3072)
2022-12-17 17:53:43,139.139 3178:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.0.bias                                      will be loaded from textual.visual_projection.0.bias                                      of shape (768,)
2022-12-17 17:53:43,140.140 3178:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.0.weight                                    will be loaded from textual.visual_projection.0.weight                                    of shape (768, 768)
2022-12-17 17:53:43,140.140 3178:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.1.bias                                      will be loaded from textual.visual_projection.1.bias                                      of shape (768,)
2022-12-17 17:53:43,140.140 3178:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.1.weight                                    will be loaded from textual.visual_projection.1.weight                                    of shape (768,)
2022-12-17 17:53:43,140.140 3178:torch_common.py:137 align_and_update_state_dicts(): target model param = 258; name matched = 258; loaded = 258
2022-12-17 17:53:43,141.141 3178:torch_common.py:140 align_and_update_state_dicts(): from loaded; ignore = []
2022-12-17 17:53:43,145.145 3178:torch_common.py:82 load_model_state_ignore_mismatch(): unique keys in init dict = ['textual.output.weight']; total = 1
2022-12-17 17:53:43,565.565 3178:torch_common.py:86 load_model_state_ignore_mismatch(): unique key (not initialized) in current model = ['textual.output.weight']
2022-12-17 18:02:13,129.129 3178:inference.py:111 test_git_inference_single_image(): output: white arrow painted on the street
2022-12-17 18:02:18,929.929 3222:inference.py:320   <module>(): param:
{'image_path': '../img38/img38_4.png',
 'model_name': 'GIT_BASE',
 'prefix': '',
 'type': 'test_git_inference_single_image'}
2022-12-17 18:02:26,271.271 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.class_embedding                                         will be loaded from image_encoder.class_embedding                                         of shape (768,)
2022-12-17 18:02:26,271.271 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.conv1.weight                                            will be loaded from image_encoder.conv1.weight                                            of shape (768, 3, 16, 16)
2022-12-17 18:02:26,271.271 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_post.bias                                            will be loaded from image_encoder.ln_post.bias                                            of shape (768,)
2022-12-17 18:02:26,271.271 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_post.weight                                          will be loaded from image_encoder.ln_post.weight                                          of shape (768,)
2022-12-17 18:02:26,272.272 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_pre.bias                                             will be loaded from image_encoder.ln_pre.bias                                             of shape (768,)
2022-12-17 18:02:26,272.272 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_pre.weight                                           will be loaded from image_encoder.ln_pre.weight                                           of shape (768,)
2022-12-17 18:02:26,272.272 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.positional_embedding                                    will be loaded from image_encoder.positional_embedding                                    of shape (197, 768)
2022-12-17 18:02:26,272.272 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.proj                                                    will be loaded from image_encoder.proj                                                    of shape (768, 512)
2022-12-17 18:02:26,272.272 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.0.attn.in_proj_bias               of shape (2304,)
2022-12-17 18:02:26,272.272 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.0.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 18:02:26,272.272 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.0.attn.out_proj.bias              of shape (768,)
2022-12-17 18:02:26,272.272 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.0.attn.out_proj.weight            of shape (768, 768)
2022-12-17 18:02:26,272.272 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.0.ln_1.bias                       of shape (768,)
2022-12-17 18:02:26,273.273 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.0.ln_1.weight                     of shape (768,)
2022-12-17 18:02:26,273.273 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.0.ln_2.bias                       of shape (768,)
2022-12-17 18:02:26,273.273 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.0.ln_2.weight                     of shape (768,)
2022-12-17 18:02:26,273.273 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.0.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 18:02:26,273.273 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.0.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 18:02:26,273.273 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.0.mlp.c_proj.bias                 of shape (768,)
2022-12-17 18:02:26,273.273 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.0.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 18:02:26,273.273 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.1.attn.in_proj_bias               of shape (2304,)
2022-12-17 18:02:26,273.273 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.1.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 18:02:26,273.273 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.1.attn.out_proj.bias              of shape (768,)
2022-12-17 18:02:26,273.273 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.1.attn.out_proj.weight            of shape (768, 768)
2022-12-17 18:02:26,273.273 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.1.ln_1.bias                       of shape (768,)
2022-12-17 18:02:26,274.274 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.1.ln_1.weight                     of shape (768,)
2022-12-17 18:02:26,274.274 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.1.ln_2.bias                       of shape (768,)
2022-12-17 18:02:26,274.274 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.1.ln_2.weight                     of shape (768,)
2022-12-17 18:02:26,274.274 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.1.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 18:02:26,274.274 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.1.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 18:02:26,274.274 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.1.mlp.c_proj.bias                 of shape (768,)
2022-12-17 18:02:26,274.274 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.1.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 18:02:26,274.274 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.in_proj_bias              will be loaded from image_encoder.transformer.resblocks.10.attn.in_proj_bias              of shape (2304,)
2022-12-17 18:02:26,274.274 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.in_proj_weight            will be loaded from image_encoder.transformer.resblocks.10.attn.in_proj_weight            of shape (2304, 768)
2022-12-17 18:02:26,274.274 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.out_proj.bias             will be loaded from image_encoder.transformer.resblocks.10.attn.out_proj.bias             of shape (768,)
2022-12-17 18:02:26,274.274 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.out_proj.weight           will be loaded from image_encoder.transformer.resblocks.10.attn.out_proj.weight           of shape (768, 768)
2022-12-17 18:02:26,274.274 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_1.bias                      will be loaded from image_encoder.transformer.resblocks.10.ln_1.bias                      of shape (768,)
2022-12-17 18:02:26,275.275 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_1.weight                    will be loaded from image_encoder.transformer.resblocks.10.ln_1.weight                    of shape (768,)
2022-12-17 18:02:26,275.275 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_2.bias                      will be loaded from image_encoder.transformer.resblocks.10.ln_2.bias                      of shape (768,)
2022-12-17 18:02:26,275.275 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_2.weight                    will be loaded from image_encoder.transformer.resblocks.10.ln_2.weight                    of shape (768,)
2022-12-17 18:02:26,275.275 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_fc.bias                  will be loaded from image_encoder.transformer.resblocks.10.mlp.c_fc.bias                  of shape (3072,)
2022-12-17 18:02:26,275.275 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_fc.weight                will be loaded from image_encoder.transformer.resblocks.10.mlp.c_fc.weight                of shape (3072, 768)
2022-12-17 18:02:26,275.275 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_proj.bias                will be loaded from image_encoder.transformer.resblocks.10.mlp.c_proj.bias                of shape (768,)
2022-12-17 18:02:26,275.275 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_proj.weight              will be loaded from image_encoder.transformer.resblocks.10.mlp.c_proj.weight              of shape (768, 3072)
2022-12-17 18:02:26,275.275 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.in_proj_bias              will be loaded from image_encoder.transformer.resblocks.11.attn.in_proj_bias              of shape (2304,)
2022-12-17 18:02:26,275.275 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.in_proj_weight            will be loaded from image_encoder.transformer.resblocks.11.attn.in_proj_weight            of shape (2304, 768)
2022-12-17 18:02:26,275.275 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.out_proj.bias             will be loaded from image_encoder.transformer.resblocks.11.attn.out_proj.bias             of shape (768,)
2022-12-17 18:02:26,276.276 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.out_proj.weight           will be loaded from image_encoder.transformer.resblocks.11.attn.out_proj.weight           of shape (768, 768)
2022-12-17 18:02:26,276.276 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_1.bias                      will be loaded from image_encoder.transformer.resblocks.11.ln_1.bias                      of shape (768,)
2022-12-17 18:02:26,276.276 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_1.weight                    will be loaded from image_encoder.transformer.resblocks.11.ln_1.weight                    of shape (768,)
2022-12-17 18:02:26,276.276 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_2.bias                      will be loaded from image_encoder.transformer.resblocks.11.ln_2.bias                      of shape (768,)
2022-12-17 18:02:26,276.276 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_2.weight                    will be loaded from image_encoder.transformer.resblocks.11.ln_2.weight                    of shape (768,)
2022-12-17 18:02:26,276.276 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_fc.bias                  will be loaded from image_encoder.transformer.resblocks.11.mlp.c_fc.bias                  of shape (3072,)
2022-12-17 18:02:26,276.276 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_fc.weight                will be loaded from image_encoder.transformer.resblocks.11.mlp.c_fc.weight                of shape (3072, 768)
2022-12-17 18:02:26,276.276 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_proj.bias                will be loaded from image_encoder.transformer.resblocks.11.mlp.c_proj.bias                of shape (768,)
2022-12-17 18:02:26,276.276 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_proj.weight              will be loaded from image_encoder.transformer.resblocks.11.mlp.c_proj.weight              of shape (768, 3072)
2022-12-17 18:02:26,276.276 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.2.attn.in_proj_bias               of shape (2304,)
2022-12-17 18:02:26,277.277 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.2.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 18:02:26,277.277 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.2.attn.out_proj.bias              of shape (768,)
2022-12-17 18:02:26,277.277 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.2.attn.out_proj.weight            of shape (768, 768)
2022-12-17 18:02:26,277.277 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.2.ln_1.bias                       of shape (768,)
2022-12-17 18:02:26,277.277 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.2.ln_1.weight                     of shape (768,)
2022-12-17 18:02:26,277.277 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.2.ln_2.bias                       of shape (768,)
2022-12-17 18:02:26,277.277 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.2.ln_2.weight                     of shape (768,)
2022-12-17 18:02:26,277.277 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.2.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 18:02:26,277.277 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.2.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 18:02:26,277.277 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.2.mlp.c_proj.bias                 of shape (768,)
2022-12-17 18:02:26,277.277 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.2.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 18:02:26,278.278 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.3.attn.in_proj_bias               of shape (2304,)
2022-12-17 18:02:26,278.278 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.3.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 18:02:26,278.278 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.3.attn.out_proj.bias              of shape (768,)
2022-12-17 18:02:26,278.278 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.3.attn.out_proj.weight            of shape (768, 768)
2022-12-17 18:02:26,278.278 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.3.ln_1.bias                       of shape (768,)
2022-12-17 18:02:26,278.278 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.3.ln_1.weight                     of shape (768,)
2022-12-17 18:02:26,278.278 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.3.ln_2.bias                       of shape (768,)
2022-12-17 18:02:26,278.278 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.3.ln_2.weight                     of shape (768,)
2022-12-17 18:02:26,278.278 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.3.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 18:02:26,278.278 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.3.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 18:02:26,279.279 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.3.mlp.c_proj.bias                 of shape (768,)
2022-12-17 18:02:26,279.279 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.3.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 18:02:26,279.279 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.4.attn.in_proj_bias               of shape (2304,)
2022-12-17 18:02:26,279.279 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.4.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 18:02:26,279.279 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.4.attn.out_proj.bias              of shape (768,)
2022-12-17 18:02:26,279.279 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.4.attn.out_proj.weight            of shape (768, 768)
2022-12-17 18:02:26,279.279 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.4.ln_1.bias                       of shape (768,)
2022-12-17 18:02:26,279.279 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.4.ln_1.weight                     of shape (768,)
2022-12-17 18:02:26,279.279 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.4.ln_2.bias                       of shape (768,)
2022-12-17 18:02:26,279.279 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.4.ln_2.weight                     of shape (768,)
2022-12-17 18:02:26,279.279 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.4.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 18:02:26,280.280 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.4.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 18:02:26,280.280 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.4.mlp.c_proj.bias                 of shape (768,)
2022-12-17 18:02:26,280.280 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.4.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 18:02:26,280.280 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.5.attn.in_proj_bias               of shape (2304,)
2022-12-17 18:02:26,280.280 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.5.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 18:02:26,280.280 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.5.attn.out_proj.bias              of shape (768,)
2022-12-17 18:02:26,280.280 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.5.attn.out_proj.weight            of shape (768, 768)
2022-12-17 18:02:26,280.280 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.5.ln_1.bias                       of shape (768,)
2022-12-17 18:02:26,280.280 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.5.ln_1.weight                     of shape (768,)
2022-12-17 18:02:26,281.281 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.5.ln_2.bias                       of shape (768,)
2022-12-17 18:02:26,281.281 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.5.ln_2.weight                     of shape (768,)
2022-12-17 18:02:26,281.281 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.5.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 18:02:26,281.281 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.5.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 18:02:26,281.281 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.5.mlp.c_proj.bias                 of shape (768,)
2022-12-17 18:02:26,281.281 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.5.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 18:02:26,281.281 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.6.attn.in_proj_bias               of shape (2304,)
2022-12-17 18:02:26,281.281 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.6.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 18:02:26,281.281 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.6.attn.out_proj.bias              of shape (768,)
2022-12-17 18:02:26,282.282 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.6.attn.out_proj.weight            of shape (768, 768)
2022-12-17 18:02:26,282.282 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.6.ln_1.bias                       of shape (768,)
2022-12-17 18:02:26,282.282 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.6.ln_1.weight                     of shape (768,)
2022-12-17 18:02:26,282.282 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.6.ln_2.bias                       of shape (768,)
2022-12-17 18:02:26,282.282 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.6.ln_2.weight                     of shape (768,)
2022-12-17 18:02:26,282.282 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.6.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 18:02:26,282.282 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.6.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 18:02:26,282.282 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.6.mlp.c_proj.bias                 of shape (768,)
2022-12-17 18:02:26,282.282 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.6.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 18:02:26,282.282 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.7.attn.in_proj_bias               of shape (2304,)
2022-12-17 18:02:26,282.282 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.7.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 18:02:26,282.282 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.7.attn.out_proj.bias              of shape (768,)
2022-12-17 18:02:26,283.283 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.7.attn.out_proj.weight            of shape (768, 768)
2022-12-17 18:02:26,283.283 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.7.ln_1.bias                       of shape (768,)
2022-12-17 18:02:26,283.283 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.7.ln_1.weight                     of shape (768,)
2022-12-17 18:02:26,283.283 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.7.ln_2.bias                       of shape (768,)
2022-12-17 18:02:26,283.283 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.7.ln_2.weight                     of shape (768,)
2022-12-17 18:02:26,283.283 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.7.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 18:02:26,283.283 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.7.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 18:02:26,283.283 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.7.mlp.c_proj.bias                 of shape (768,)
2022-12-17 18:02:26,283.283 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.7.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 18:02:26,283.283 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.8.attn.in_proj_bias               of shape (2304,)
2022-12-17 18:02:26,284.284 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.8.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 18:02:26,284.284 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.8.attn.out_proj.bias              of shape (768,)
2022-12-17 18:02:26,284.284 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.8.attn.out_proj.weight            of shape (768, 768)
2022-12-17 18:02:26,284.284 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.8.ln_1.bias                       of shape (768,)
2022-12-17 18:02:26,284.284 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.8.ln_1.weight                     of shape (768,)
2022-12-17 18:02:26,284.284 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.8.ln_2.bias                       of shape (768,)
2022-12-17 18:02:26,284.284 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.8.ln_2.weight                     of shape (768,)
2022-12-17 18:02:26,284.284 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.8.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 18:02:26,284.284 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.8.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 18:02:26,284.284 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.8.mlp.c_proj.bias                 of shape (768,)
2022-12-17 18:02:26,285.285 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.8.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 18:02:26,285.285 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.9.attn.in_proj_bias               of shape (2304,)
2022-12-17 18:02:26,285.285 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.9.attn.in_proj_weight             of shape (2304, 768)
2022-12-17 18:02:26,285.285 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.9.attn.out_proj.bias              of shape (768,)
2022-12-17 18:02:26,285.285 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.9.attn.out_proj.weight            of shape (768, 768)
2022-12-17 18:02:26,285.285 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.9.ln_1.bias                       of shape (768,)
2022-12-17 18:02:26,285.285 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.9.ln_1.weight                     of shape (768,)
2022-12-17 18:02:26,285.285 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.9.ln_2.bias                       of shape (768,)
2022-12-17 18:02:26,285.285 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.9.ln_2.weight                     of shape (768,)
2022-12-17 18:02:26,285.285 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.9.mlp.c_fc.bias                   of shape (3072,)
2022-12-17 18:02:26,286.286 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.9.mlp.c_fc.weight                 of shape (3072, 768)
2022-12-17 18:02:26,286.286 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.9.mlp.c_proj.bias                 of shape (768,)
2022-12-17 18:02:26,286.286 3222:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.9.mlp.c_proj.weight               of shape (768, 3072)
2022-12-17 18:02:26,286.286 3222:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.layer_norm.bias                                     will be loaded from textual.embedding.layer_norm.bias                                     of shape (768,)
2022-12-17 18:02:26,286.286 3222:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.layer_norm.weight                                   will be loaded from textual.embedding.layer_norm.weight                                   of shape (768,)
2022-12-17 18:02:26,286.286 3222:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.positions.weight                                    will be loaded from textual.embedding.positions.weight                                    of shape (1024, 768)
2022-12-17 18:02:26,286.286 3222:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.words.weight                                        will be loaded from textual.embedding.words.weight                                        of shape (30522, 768)
2022-12-17 18:02:26,286.286 3222:torch_common.py:128 align_and_update_state_dicts(): textual.output.bias                                                   will be loaded from textual.output.bias                                                   of shape (30522,)
2022-12-17 18:02:26,286.286 3222:torch_common.py:128 align_and_update_state_dicts(): textual.output.weight                                                 will be loaded from textual.output.weight                                                 of shape (30522, 768)
2022-12-17 18:02:26,286.286 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.0.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 18:02:26,286.286 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.0.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 18:02:26,287.287 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.0.attention.output.dense.bias       of shape (768,)
2022-12-17 18:02:26,287.287 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.0.attention.output.dense.weight     of shape (768, 768)
2022-12-17 18:02:26,287.287 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.0.attention.self.key.bias           of shape (768,)
2022-12-17 18:02:26,287.287 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.0.attention.self.key.weight         of shape (768, 768)
2022-12-17 18:02:26,287.287 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.0.attention.self.query.bias         of shape (768,)
2022-12-17 18:02:26,287.287 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.0.attention.self.query.weight       of shape (768, 768)
2022-12-17 18:02:26,287.287 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.0.attention.self.value.bias         of shape (768,)
2022-12-17 18:02:26,287.287 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.0.attention.self.value.weight       of shape (768, 768)
2022-12-17 18:02:26,287.287 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.0.intermediate.dense.bias           of shape (3072,)
2022-12-17 18:02:26,287.287 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.0.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 18:02:26,287.287 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.0.output.LayerNorm.bias             of shape (768,)
2022-12-17 18:02:26,288.288 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.0.output.LayerNorm.weight           of shape (768,)
2022-12-17 18:02:26,288.288 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.0.output.dense.bias                 of shape (768,)
2022-12-17 18:02:26,288.288 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.dense.weight               will be loaded from textual.transformer.encoder.layer.0.output.dense.weight               of shape (768, 3072)
2022-12-17 18:02:26,288.288 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.1.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 18:02:26,288.288 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.1.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 18:02:26,288.288 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.1.attention.output.dense.bias       of shape (768,)
2022-12-17 18:02:26,288.288 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.1.attention.output.dense.weight     of shape (768, 768)
2022-12-17 18:02:26,288.288 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.1.attention.self.key.bias           of shape (768,)
2022-12-17 18:02:26,288.288 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.1.attention.self.key.weight         of shape (768, 768)
2022-12-17 18:02:26,288.288 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.1.attention.self.query.bias         of shape (768,)
2022-12-17 18:02:26,288.288 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.1.attention.self.query.weight       of shape (768, 768)
2022-12-17 18:02:26,289.289 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.1.attention.self.value.bias         of shape (768,)
2022-12-17 18:02:26,289.289 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.1.attention.self.value.weight       of shape (768, 768)
2022-12-17 18:02:26,289.289 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.1.intermediate.dense.bias           of shape (3072,)
2022-12-17 18:02:26,289.289 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.1.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 18:02:26,289.289 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.1.output.LayerNorm.bias             of shape (768,)
2022-12-17 18:02:26,289.289 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.1.output.LayerNorm.weight           of shape (768,)
2022-12-17 18:02:26,289.289 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.1.output.dense.bias                 of shape (768,)
2022-12-17 18:02:26,289.289 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.dense.weight               will be loaded from textual.transformer.encoder.layer.1.output.dense.weight               of shape (768, 3072)
2022-12-17 18:02:26,289.289 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.2.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 18:02:26,289.289 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.2.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 18:02:26,290.290 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.2.attention.output.dense.bias       of shape (768,)
2022-12-17 18:02:26,290.290 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.2.attention.output.dense.weight     of shape (768, 768)
2022-12-17 18:02:26,290.290 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.2.attention.self.key.bias           of shape (768,)
2022-12-17 18:02:26,290.290 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.2.attention.self.key.weight         of shape (768, 768)
2022-12-17 18:02:26,290.290 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.2.attention.self.query.bias         of shape (768,)
2022-12-17 18:02:26,290.290 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.2.attention.self.query.weight       of shape (768, 768)
2022-12-17 18:02:26,290.290 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.2.attention.self.value.bias         of shape (768,)
2022-12-17 18:02:26,290.290 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.2.attention.self.value.weight       of shape (768, 768)
2022-12-17 18:02:26,290.290 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.2.intermediate.dense.bias           of shape (3072,)
2022-12-17 18:02:26,291.291 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.2.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 18:02:26,291.291 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.2.output.LayerNorm.bias             of shape (768,)
2022-12-17 18:02:26,291.291 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.2.output.LayerNorm.weight           of shape (768,)
2022-12-17 18:02:26,291.291 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.2.output.dense.bias                 of shape (768,)
2022-12-17 18:02:26,291.291 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.dense.weight               will be loaded from textual.transformer.encoder.layer.2.output.dense.weight               of shape (768, 3072)
2022-12-17 18:02:26,291.291 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.3.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 18:02:26,291.291 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.3.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 18:02:26,291.291 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.3.attention.output.dense.bias       of shape (768,)
2022-12-17 18:02:26,291.291 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.3.attention.output.dense.weight     of shape (768, 768)
2022-12-17 18:02:26,291.291 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.3.attention.self.key.bias           of shape (768,)
2022-12-17 18:02:26,291.291 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.3.attention.self.key.weight         of shape (768, 768)
2022-12-17 18:02:26,291.291 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.3.attention.self.query.bias         of shape (768,)
2022-12-17 18:02:26,292.292 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.3.attention.self.query.weight       of shape (768, 768)
2022-12-17 18:02:26,292.292 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.3.attention.self.value.bias         of shape (768,)
2022-12-17 18:02:26,292.292 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.3.attention.self.value.weight       of shape (768, 768)
2022-12-17 18:02:26,292.292 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.3.intermediate.dense.bias           of shape (3072,)
2022-12-17 18:02:26,292.292 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.3.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 18:02:26,292.292 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.3.output.LayerNorm.bias             of shape (768,)
2022-12-17 18:02:26,292.292 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.3.output.LayerNorm.weight           of shape (768,)
2022-12-17 18:02:26,292.292 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.3.output.dense.bias                 of shape (768,)
2022-12-17 18:02:26,292.292 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.dense.weight               will be loaded from textual.transformer.encoder.layer.3.output.dense.weight               of shape (768, 3072)
2022-12-17 18:02:26,292.292 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.4.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 18:02:26,292.292 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.4.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 18:02:26,293.293 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.4.attention.output.dense.bias       of shape (768,)
2022-12-17 18:02:26,293.293 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.4.attention.output.dense.weight     of shape (768, 768)
2022-12-17 18:02:26,293.293 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.4.attention.self.key.bias           of shape (768,)
2022-12-17 18:02:26,293.293 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.4.attention.self.key.weight         of shape (768, 768)
2022-12-17 18:02:26,293.293 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.4.attention.self.query.bias         of shape (768,)
2022-12-17 18:02:26,293.293 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.4.attention.self.query.weight       of shape (768, 768)
2022-12-17 18:02:26,293.293 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.4.attention.self.value.bias         of shape (768,)
2022-12-17 18:02:26,293.293 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.4.attention.self.value.weight       of shape (768, 768)
2022-12-17 18:02:26,293.293 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.4.intermediate.dense.bias           of shape (3072,)
2022-12-17 18:02:26,294.294 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.4.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 18:02:26,294.294 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.4.output.LayerNorm.bias             of shape (768,)
2022-12-17 18:02:26,294.294 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.4.output.LayerNorm.weight           of shape (768,)
2022-12-17 18:02:26,294.294 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.4.output.dense.bias                 of shape (768,)
2022-12-17 18:02:26,294.294 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.dense.weight               will be loaded from textual.transformer.encoder.layer.4.output.dense.weight               of shape (768, 3072)
2022-12-17 18:02:26,294.294 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.5.attention.output.LayerNorm.bias   of shape (768,)
2022-12-17 18:02:26,294.294 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.5.attention.output.LayerNorm.weight of shape (768,)
2022-12-17 18:02:26,294.294 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.5.attention.output.dense.bias       of shape (768,)
2022-12-17 18:02:26,294.294 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.5.attention.output.dense.weight     of shape (768, 768)
2022-12-17 18:02:26,294.294 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.5.attention.self.key.bias           of shape (768,)
2022-12-17 18:02:26,294.294 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.5.attention.self.key.weight         of shape (768, 768)
2022-12-17 18:02:26,295.295 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.5.attention.self.query.bias         of shape (768,)
2022-12-17 18:02:26,295.295 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.5.attention.self.query.weight       of shape (768, 768)
2022-12-17 18:02:26,295.295 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.5.attention.self.value.bias         of shape (768,)
2022-12-17 18:02:26,295.295 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.5.attention.self.value.weight       of shape (768, 768)
2022-12-17 18:02:26,295.295 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.5.intermediate.dense.bias           of shape (3072,)
2022-12-17 18:02:26,295.295 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.5.intermediate.dense.weight         of shape (3072, 768)
2022-12-17 18:02:26,295.295 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.5.output.LayerNorm.bias             of shape (768,)
2022-12-17 18:02:26,295.295 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.5.output.LayerNorm.weight           of shape (768,)
2022-12-17 18:02:26,295.295 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.5.output.dense.bias                 of shape (768,)
2022-12-17 18:02:26,296.296 3222:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.dense.weight               will be loaded from textual.transformer.encoder.layer.5.output.dense.weight               of shape (768, 3072)
2022-12-17 18:02:26,296.296 3222:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.0.bias                                      will be loaded from textual.visual_projection.0.bias                                      of shape (768,)
2022-12-17 18:02:26,296.296 3222:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.0.weight                                    will be loaded from textual.visual_projection.0.weight                                    of shape (768, 768)
2022-12-17 18:02:26,296.296 3222:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.1.bias                                      will be loaded from textual.visual_projection.1.bias                                      of shape (768,)
2022-12-17 18:02:26,296.296 3222:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.1.weight                                    will be loaded from textual.visual_projection.1.weight                                    of shape (768,)
2022-12-17 18:02:26,296.296 3222:torch_common.py:137 align_and_update_state_dicts(): target model param = 258; name matched = 258; loaded = 258
2022-12-17 18:02:26,296.296 3222:torch_common.py:140 align_and_update_state_dicts(): from loaded; ignore = []
2022-12-17 18:02:26,298.298 3222:torch_common.py:82 load_model_state_ignore_mismatch(): unique keys in init dict = ['textual.output.weight']; total = 1
2022-12-17 18:02:26,396.396 3222:torch_common.py:86 load_model_state_ignore_mismatch(): unique key (not initialized) in current model = ['textual.output.weight']
2022-12-17 18:03:01,631.631 3222:inference.py:111 test_git_inference_single_image(): output: white lines on the road
